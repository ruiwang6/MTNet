ssh://root@region-42.seetacloud.com:17240/root/miniconda3/bin/python -u /project/DSTAGNN-main/train_DSTAGNN_my.py
Read configuration file: configurations/PEMS04_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS04/PEMS04_r1_d0_w0_dstagnn
train: torch.Size([10181, 307, 1, 12]) torch.Size([10181, 307, 12])
val: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
test: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
delete the old one and create params directory myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 64
graph_signal_matrix_filename	 ./data/PEMS04/PEMS04.npz
start_epoch	 0
epochs	 110
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.0.EmbedT.norm.weight 	 torch.Size([307])
BlockList.0.EmbedT.norm.bias 	 torch.Size([307])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.0.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.1.EmbedT.norm.weight 	 torch.Size([307])
BlockList.1.EmbedT.norm.bias 	 torch.Size([307])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.1.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.2.EmbedT.norm.weight 	 torch.Size([307])
BlockList.2.EmbedT.norm.bias 	 torch.Size([307])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.2.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.3.EmbedT.norm.weight 	 torch.Size([307])
BlockList.3.EmbedT.norm.bias 	 torch.Size([307])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.3.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 3579728
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 54, loss: 60.815922
val loss 62.36921465838397
best epoch:  0
best val loss:  62.36921465838397
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 54, loss: 2.112269
val loss 2.5400526611893266
best epoch:  1
best val loss:  2.5400526611893266
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 54, loss: 1.902900
val loss 2.054120099103009
best epoch:  2
best val loss:  2.054120099103009
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 54, loss: 1.797568
val loss 1.9103240194144073
best epoch:  3
best val loss:  1.9103240194144073
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 54, loss: 1.797696
val loss 1.8752669934873227
best epoch:  4
best val loss:  1.8752669934873227
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 54, loss: 1.770104
val loss 1.8460482160250347
best epoch:  5
best val loss:  1.8460482160250347
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 54, loss: 1.737737
val loss 1.804322079375938
best epoch:  6
best val loss:  1.804322079375938
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
global step: 1000, training loss: 1.612798, time: 1.298955s
current epoch:  7
validation batch 1 / 54, loss: 1.673355
val loss 1.7247825816825584
best epoch:  7
best val loss:  1.7247825816825584
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 54, loss: 1.637881
val loss 1.6613400964825242
best epoch:  8
best val loss:  1.6613400964825242
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 54, loss: 1.622236
val loss 1.648072060611513
best epoch:  9
best val loss:  1.648072060611513
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 54, loss: 1.568609
val loss 1.5868976944022708
best epoch:  10
best val loss:  1.5868976944022708
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 54, loss: 1.570697
val loss 1.5657184929759413
best epoch:  11
best val loss:  1.5657184929759413
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 54, loss: 1.491955
val loss 1.50230257930579
best epoch:  12
best val loss:  1.50230257930579
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
global step: 2000, training loss: 1.180400, time: 1.298184s
current epoch:  13
validation batch 1 / 54, loss: 1.547742
val loss 1.5462757371090077
current epoch:  14
validation batch 1 / 54, loss: 1.457969
val loss 1.467573453982671
best epoch:  14
best val loss:  1.467573453982671
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
current epoch:  15
validation batch 1 / 54, loss: 1.470047
val loss 1.4591541643495913
best epoch:  15
best val loss:  1.4591541643495913
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 54, loss: 1.477329
val loss 1.48168495076674
current epoch:  17
validation batch 1 / 54, loss: 1.450626
val loss 1.4363935887813568
best epoch:  17
best val loss:  1.4363935887813568
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
current epoch:  18
validation batch 1 / 54, loss: 1.430371
val loss 1.4331771974210386
best epoch:  18
best val loss:  1.4331771974210386
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_18.params
global step: 3000, training loss: 1.082263, time: 1.295980s
current epoch:  19
validation batch 1 / 54, loss: 1.416458
val loss 1.4078690144750807
best epoch:  19
best val loss:  1.4078690144750807
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 54, loss: 1.402484
val loss 1.3920505731194108
best epoch:  20
best val loss:  1.3920505731194108
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_20.params
current epoch:  21
validation batch 1 / 54, loss: 1.403953
val loss 1.3829134117673945
best epoch:  21
best val loss:  1.3829134117673945
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_21.params
current epoch:  22
validation batch 1 / 54, loss: 1.410258
val loss 1.3926314910252888
current epoch:  23
validation batch 1 / 54, loss: 1.408017
val loss 1.4017025309580344
current epoch:  24
validation batch 1 / 54, loss: 1.393166
val loss 1.383341450382162
global step: 4000, training loss: 0.639062, time: 0.297566s
current epoch:  25
validation batch 1 / 54, loss: 1.402571
val loss 1.387907186040172
current epoch:  26
validation batch 1 / 54, loss: 1.393373
val loss 1.3710712680110224
best epoch:  26
best val loss:  1.3710712680110224
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_26.params
current epoch:  27
validation batch 1 / 54, loss: 1.396838
val loss 1.376256944956603
current epoch:  28
validation batch 1 / 54, loss: 1.377145
val loss 1.355538414032371
best epoch:  28
best val loss:  1.355538414032371
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_28.params
current epoch:  29
validation batch 1 / 54, loss: 1.397317
val loss 1.3855736542631079
current epoch:  30
validation batch 1 / 54, loss: 1.397603
val loss 1.3756233740735937
current epoch:  31
validation batch 1 / 54, loss: 1.395765
val loss 1.3719463685044535
global step: 5000, training loss: 0.963753, time: 1.287208s
current epoch:  32
validation batch 1 / 54, loss: 1.383802
val loss 1.3575094005575887
current epoch:  33
validation batch 1 / 54, loss: 1.399200
val loss 1.3757130669222937
current epoch:  34
validation batch 1 / 54, loss: 1.394499
val loss 1.3663390345043607
current epoch:  35
validation batch 1 / 54, loss: 1.385819
val loss 1.362722357114156
current epoch:  36
validation batch 1 / 54, loss: 1.385673
val loss 1.346509317005122
best epoch:  36
best val loss:  1.346509317005122
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_36.params
current epoch:  37
validation batch 1 / 54, loss: 1.386705
val loss 1.352367087646767
global step: 6000, training loss: 0.926043, time: 1.301583s
current epoch:  38
validation batch 1 / 54, loss: 1.388170
val loss 1.360538457830747
current epoch:  39
validation batch 1 / 54, loss: 1.397512
val loss 1.3540301085622222
current epoch:  40
validation batch 1 / 54, loss: 1.383999
val loss 1.3554252518547907
current epoch:  41
validation batch 1 / 54, loss: 1.387056
val loss 1.343942195728973
best epoch:  41
best val loss:  1.343942195728973
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_41.params
current epoch:  42
validation batch 1 / 54, loss: 1.383237
val loss 1.3469639111448217
current epoch:  43
validation batch 1 / 54, loss: 1.392979
val loss 1.3567733339689396
global step: 7000, training loss: 0.780928, time: 1.311374s
current epoch:  44
validation batch 1 / 54, loss: 1.382170
val loss 1.3530846972156454
current epoch:  45
validation batch 1 / 54, loss: 1.378495
val loss 1.3514293007276676
current epoch:  46
validation batch 1 / 54, loss: 1.388155
val loss 1.3510198411014345
current epoch:  47
validation batch 1 / 54, loss: 1.382572
val loss 1.3479078274082255
current epoch:  48
validation batch 1 / 54, loss: 1.389104
val loss 1.363439037292092
current epoch:  49
validation batch 1 / 54, loss: 1.390819
val loss 1.366787126770726
global step: 8000, training loss: 0.748326, time: 0.304397s
current epoch:  50
validation batch 1 / 54, loss: 1.392789
val loss 1.3669866455925836
current epoch:  51
validation batch 1 / 54, loss: 1.394522
val loss 1.3563531786203384
current epoch:  52
validation batch 1 / 54, loss: 1.383296
val loss 1.3552789643958763
current epoch:  53
validation batch 1 / 54, loss: 1.384122
val loss 1.3679452290137608
current epoch:  54
validation batch 1 / 54, loss: 1.388397
val loss 1.3584520325616554
current epoch:  55
validation batch 1 / 54, loss: 1.388008
val loss 1.3681809157133102
current epoch:  56
validation batch 1 / 54, loss: 1.389967
val loss 1.360954854775358
global step: 9000, training loss: 0.857350, time: 1.296420s
current epoch:  57
validation batch 1 / 54, loss: 1.384802
val loss 1.3643193283566721
current epoch:  58
validation batch 1 / 54, loss: 1.377728
val loss 1.359868049621582
current epoch:  59
validation batch 1 / 54, loss: 1.408639
val loss 1.3762455334266026
current epoch:  60
validation batch 1 / 54, loss: 1.400942
val loss 1.3631502721044753
current epoch:  61
validation batch 1 / 54, loss: 1.384628
val loss 1.3600415659171563
current epoch:  62
validation batch 1 / 54, loss: 1.400174
val loss 1.365076638482235
global step: 10000, training loss: 0.824602, time: 1.304502s
current epoch:  63
validation batch 1 / 54, loss: 1.398447
val loss 1.3696111826984971
current epoch:  64
validation batch 1 / 54, loss: 1.401524
val loss 1.3677180216268257
current epoch:  65
validation batch 1 / 54, loss: 1.408127
val loss 1.3780113872554567
current epoch:  66
validation batch 1 / 54, loss: 1.402665
val loss 1.370202225115564
current epoch:  67
validation batch 1 / 54, loss: 1.401498
val loss 1.375299980794942
current epoch:  68
validation batch 1 / 54, loss: 1.398530
val loss 1.3712067515761763
global step: 11000, training loss: 0.849460, time: 1.298998s
current epoch:  69
validation batch 1 / 54, loss: 1.396362
val loss 1.3688932342661753
current epoch:  70
validation batch 1 / 54, loss: 1.396895
val loss 1.3719571922664289
current epoch:  71
validation batch 1 / 54, loss: 1.413080
val loss 1.3758950277611062
current epoch:  72
validation batch 1 / 54, loss: 1.395707
val loss 1.371348653126646
current epoch:  73
validation batch 1 / 54, loss: 1.401872
val loss 1.380977972238152
current epoch:  74
validation batch 1 / 54, loss: 1.401937
val loss 1.3739306054733418
global step: 12000, training loss: 0.489955, time: 0.400402s
current epoch:  75
validation batch 1 / 54, loss: 1.403403
val loss 1.381678327366158
current epoch:  76
validation batch 1 / 54, loss: 1.392472
val loss 1.3723476727803547
current epoch:  77
validation batch 1 / 54, loss: 1.407029
val loss 1.376264445207737
current epoch:  78
validation batch 1 / 54, loss: 1.396694
val loss 1.3805343525277243
current epoch:  79
validation batch 1 / 54, loss: 1.404800
val loss 1.3798533413145277
current epoch:  80
validation batch 1 / 54, loss: 1.402232
val loss 1.3794068975581064
current epoch:  81
validation batch 1 / 54, loss: 1.409432
val loss 1.3887263906222802
global step: 13000, training loss: 0.820703, time: 1.299038s
current epoch:  82
validation batch 1 / 54, loss: 1.402601
val loss 1.3793702379420951
current epoch:  83
validation batch 1 / 54, loss: 1.419599
val loss 1.3882039106554456
current epoch:  84
validation batch 1 / 54, loss: 1.417818
val loss 1.3870896183782153
current epoch:  85
validation batch 1 / 54, loss: 1.411252
val loss 1.3874430728179437
current epoch:  86
validation batch 1 / 54, loss: 1.422422
val loss 1.3941463000244565
current epoch:  87
validation batch 1 / 54, loss: 1.413091
val loss 1.3926191340993952
global step: 14000, training loss: 0.762771, time: 1.306678s
current epoch:  88
validation batch 1 / 54, loss: 1.442490
val loss 1.3987356705798044
current epoch:  89
validation batch 1 / 54, loss: 1.411945
val loss 1.3868275753877781
current epoch:  90
validation batch 1 / 54, loss: 1.421123
val loss 1.3877750866942935
current epoch:  91
validation batch 1 / 54, loss: 1.417677
val loss 1.3951450244144157
current epoch:  92
validation batch 1 / 54, loss: 1.409609
val loss 1.3896485224918083
current epoch:  93
validation batch 1 / 54, loss: 1.425881
val loss 1.39346358511183
global step: 15000, training loss: 0.753061, time: 1.299536s
current epoch:  94
validation batch 1 / 54, loss: 1.420739
val loss 1.3938354971232239
current epoch:  95
validation batch 1 / 54, loss: 1.421589
val loss 1.3916811369083546
current epoch:  96
validation batch 1 / 54, loss: 1.433423
val loss 1.404948971889637
current epoch:  97
validation batch 1 / 54, loss: 1.420020
val loss 1.3983164175792977
current epoch:  98
validation batch 1 / 54, loss: 1.431814
val loss 1.400159493640617
current epoch:  99
validation batch 1 / 54, loss: 1.433572
val loss 1.394464631875356
global step: 16000, training loss: 1.005544, time: 0.396984s
current epoch:  100
validation batch 1 / 54, loss: 1.426584
val loss 1.401123395120656
current epoch:  101
validation batch 1 / 54, loss: 1.426955
val loss 1.4044565873013601
current epoch:  102
validation batch 1 / 54, loss: 1.431473
val loss 1.4028703023989995
current epoch:  103
validation batch 1 / 54, loss: 1.435302
val loss 1.4010658181375928
current epoch:  104
validation batch 1 / 54, loss: 1.435834
val loss 1.407440056403478
current epoch:  105
validation batch 1 / 54, loss: 1.446959
val loss 1.4059732285914597
current epoch:  106
validation batch 1 / 54, loss: 1.445402
val loss 1.411128067859897
global step: 17000, training loss: 0.751216, time: 1.296950s
current epoch:  107
validation batch 1 / 54, loss: 1.437426
val loss 1.4110200509980872
current epoch:  108
validation batch 1 / 54, loss: 1.434980
val loss 1.410552512716364
current epoch:  109
validation batch 1 / 54, loss: 1.431280
val loss 1.4074562970134947
best epoch: 41
Average Training Time: 77.3552 secs/epoch
Average Inference Time: 29.8835 secs
best epoch: 41
load weight from: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_41.params
predicting data set batch 1 / 54
input: (3394, 307, 1, 12)
prediction: (3394, 307, 12)
data_target_tensor: (3394, 307, 12)
current epoch: 41, predict 1-th point
MAE: 0.995283
RMSE: 1.973232
MAPE: 2.031889
current epoch: 41, predict 2-th point
MAE: 1.262362
RMSE: 2.591986
MAPE: 2.593584
current epoch: 41, predict 3-th point
MAE: 1.410027
RMSE: 3.022215
MAPE: 2.951366
current epoch: 41, predict 4-th point
MAE: 1.539791
RMSE: 3.381513
MAPE: 3.278892
current epoch: 41, predict 5-th point
MAE: 1.640839
RMSE: 3.670728
MAPE: 3.544075
current epoch: 41, predict 6-th point
MAE: 1.734331
RMSE: 3.942788
MAPE: 3.804030
current epoch: 41, predict 7-th point
MAE: 1.810929
RMSE: 4.150937
MAPE: 4.010294
current epoch: 41, predict 8-th point
MAE: 1.864192
RMSE: 4.308877
MAPE: 4.150923
current epoch: 41, predict 9-th point
MAE: 1.916938
RMSE: 4.457385
MAPE: 4.291239
current epoch: 41, predict 10-th point
MAE: 1.965248
RMSE: 4.577881
MAPE: 4.414347
current epoch: 41, predict 11-th point
MAE: 2.013829
RMSE: 4.695768
MAPE: 4.535195
current epoch: 41, predict 12-th point
MAE: 2.065180
RMSE: 4.804860
MAPE: 4.661454
all MAE: 1.684911
all RMSE: 3.894004
all MAPE: 3.688940
[0.9952826, 1.9732320115243218, 2.03188918530941, 1.2623618, 2.591986458807766, 2.593584358692169, 1.4100267, 3.0222154459067028, 2.9513660818338394, 1.5397907, 3.381513033241471, 3.2788921147584915, 1.6408393, 3.6707283989366153, 3.5440754145383835, 1.7343315, 3.94278797924202, 3.804030269384384, 1.8109286, 4.150936544978669, 4.0102943778038025, 1.8641918, 4.30887728023643, 4.15092259645462, 1.9169377, 4.457385143569629, 4.291239380836487, 1.9652483, 4.577881269509358, 4.414346814155579, 2.0138295, 4.6957680581178245, 4.535194858908653, 2.06518, 4.804859784766828, 4.661453515291214, 1.6849115, 3.8940042428031263, 3.688940405845642]

Process finished with exit code -1
