ssh://root@region-42.seetacloud.com:17240/root/miniconda3/bin/python -u /project/DSTAGNN-main/train_DSTAGNN_my.py
Read configuration file: configurations/PEMS04_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS04/PEMS04_r1_d0_w0_dstagnn
train: torch.Size([10181, 307, 1, 12]) torch.Size([10181, 307, 12])
val: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
test: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
delete the old one and create params directory myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 64
graph_signal_matrix_filename	 ./data/PEMS04/PEMS04.npz
start_epoch	 0
epochs	 110
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.0.EmbedT.norm.weight 	 torch.Size([307])
BlockList.0.EmbedT.norm.bias 	 torch.Size([307])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.0.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.1.EmbedT.norm.weight 	 torch.Size([307])
BlockList.1.EmbedT.norm.bias 	 torch.Size([307])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.1.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.2.EmbedT.norm.weight 	 torch.Size([307])
BlockList.2.EmbedT.norm.bias 	 torch.Size([307])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.2.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.3.EmbedT.norm.weight 	 torch.Size([307])
BlockList.3.EmbedT.norm.bias 	 torch.Size([307])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.3.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 3579728
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 54, loss: 287.193085
val loss 216.0973765761764
best epoch:  0
best val loss:  216.0973765761764
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 54, loss: 167.892441
val loss 128.10509837115254
best epoch:  1
best val loss:  128.10509837115254
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 54, loss: 54.078552
val loss 45.32025861740112
best epoch:  2
best val loss:  45.32025861740112
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 54, loss: 33.607548
val loss 28.864342459925897
best epoch:  3
best val loss:  28.864342459925897
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 54, loss: 30.163147
val loss 25.668481526551425
best epoch:  4
best val loss:  25.668481526551425
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 54, loss: 28.806589
val loss 24.48847276193124
best epoch:  5
best val loss:  24.48847276193124
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 54, loss: 28.603325
val loss 24.089815492983217
best epoch:  6
best val loss:  24.089815492983217
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
global step: 1000, training loss: 24.341194, time: 1.303017s
current epoch:  7
validation batch 1 / 54, loss: 27.676481
val loss 22.775517251756455
best epoch:  7
best val loss:  22.775517251756455
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 54, loss: 27.099264
val loss 22.22580129128915
best epoch:  8
best val loss:  22.22580129128915
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 54, loss: 26.735071
val loss 21.814014982294154
best epoch:  9
best val loss:  21.814014982294154
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 54, loss: 26.472507
val loss 21.189445054089582
best epoch:  10
best val loss:  21.189445054089582
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 54, loss: 26.132673
val loss 21.004800390314173
best epoch:  11
best val loss:  21.004800390314173
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 54, loss: 25.915886
val loss 20.690714518229168
best epoch:  12
best val loss:  20.690714518229168
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
global step: 2000, training loss: 21.247808, time: 1.297331s
current epoch:  13
validation batch 1 / 54, loss: 25.802986
val loss 20.692548398618346
current epoch:  14
validation batch 1 / 54, loss: 25.828468
val loss 20.433843029869927
best epoch:  14
best val loss:  20.433843029869927
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
current epoch:  15
validation batch 1 / 54, loss: 25.506701
val loss 20.325424211996573
best epoch:  15
best val loss:  20.325424211996573
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 54, loss: 25.284788
val loss 20.197813475573504
best epoch:  16
best val loss:  20.197813475573504
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_16.params
current epoch:  17
validation batch 1 / 54, loss: 25.266695
val loss 20.12583827972412
best epoch:  17
best val loss:  20.12583827972412
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
current epoch:  18
validation batch 1 / 54, loss: 25.060015
val loss 20.03934046074196
best epoch:  18
best val loss:  20.03934046074196
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_18.params
global step: 3000, training loss: 19.614544, time: 1.287505s
current epoch:  19
validation batch 1 / 54, loss: 24.975826
val loss 19.955705907609726
best epoch:  19
best val loss:  19.955705907609726
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 54, loss: 24.908669
val loss 19.88318575753106
best epoch:  20
best val loss:  19.88318575753106
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_20.params
current epoch:  21
validation batch 1 / 54, loss: 24.966045
val loss 19.66327009377656
best epoch:  21
best val loss:  19.66327009377656
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_21.params
current epoch:  22
validation batch 1 / 54, loss: 25.098057
val loss 19.625353848492658
best epoch:  22
best val loss:  19.625353848492658
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_22.params
current epoch:  23
validation batch 1 / 54, loss: 24.911261
val loss 19.492651992373997
best epoch:  23
best val loss:  19.492651992373997
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_23.params
current epoch:  24
validation batch 1 / 54, loss: 24.725889
val loss 19.48383562653153
best epoch:  24
best val loss:  19.48383562653153
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
global step: 4000, training loss: 11.911094, time: 0.305998s
current epoch:  25
validation batch 1 / 54, loss: 24.706289
val loss 19.553535214176886
current epoch:  26
validation batch 1 / 54, loss: 24.685402
val loss 19.541018380059135
current epoch:  27
validation batch 1 / 54, loss: 24.553616
val loss 19.37691209934376
best epoch:  27
best val loss:  19.37691209934376
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_27.params
current epoch:  28
validation batch 1 / 54, loss: 24.730694
val loss 19.327623084739404
best epoch:  28
best val loss:  19.327623084739404
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_28.params
current epoch:  29
validation batch 1 / 54, loss: 24.568634
val loss 19.262296773769236
best epoch:  29
best val loss:  19.262296773769236
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_29.params
current epoch:  30
validation batch 1 / 54, loss: 24.441069
val loss 19.2883074371903
current epoch:  31
validation batch 1 / 54, loss: 24.446058
val loss 19.40501516836661
global step: 5000, training loss: 17.282887, time: 1.779499s
current epoch:  32
validation batch 1 / 54, loss: 24.303297
val loss 19.240685392309118
best epoch:  32
best val loss:  19.240685392309118
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_32.params
current epoch:  33
validation batch 1 / 54, loss: 24.440701
val loss 19.157300648865878
best epoch:  33
best val loss:  19.157300648865878
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_33.params
current epoch:  34
validation batch 1 / 54, loss: 24.265091
val loss 19.174749480353462
current epoch:  35
validation batch 1 / 54, loss: 24.211731
val loss 19.322696950700546
current epoch:  36
validation batch 1 / 54, loss: 24.232071
val loss 19.160853483058787
current epoch:  37
validation batch 1 / 54, loss: 24.270426
val loss 19.110072833520395
best epoch:  37
best val loss:  19.110072833520395
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_37.params
global step: 6000, training loss: 17.185919, time: 1.302642s
current epoch:  38
validation batch 1 / 54, loss: 24.201925
val loss 19.10973596572876
best epoch:  38
best val loss:  19.10973596572876
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_38.params
current epoch:  39
validation batch 1 / 54, loss: 24.160534
val loss 19.058074712753296
best epoch:  39
best val loss:  19.058074712753296
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_39.params
current epoch:  40
validation batch 1 / 54, loss: 24.215002
val loss 19.036765381141944
best epoch:  40
best val loss:  19.036765381141944
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_40.params
current epoch:  41
validation batch 1 / 54, loss: 24.320543
val loss 18.91199278831482
best epoch:  41
best val loss:  18.91199278831482
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_41.params
current epoch:  42
validation batch 1 / 54, loss: 24.354946
val loss 18.9390347622059
current epoch:  43
validation batch 1 / 54, loss: 24.107794
val loss 18.85643117516129
best epoch:  43
best val loss:  18.85643117516129
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_43.params
global step: 7000, training loss: 16.489750, time: 1.312377s
current epoch:  44
validation batch 1 / 54, loss: 24.115990
val loss 18.832886536916096
best epoch:  44
best val loss:  18.832886536916096
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_44.params
current epoch:  45
validation batch 1 / 54, loss: 24.055616
val loss 18.818883030502885
best epoch:  45
best val loss:  18.818883030502885
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_45.params
current epoch:  46
validation batch 1 / 54, loss: 24.030809
val loss 18.85562397815563
current epoch:  47
validation batch 1 / 54, loss: 24.324280
val loss 18.755470055120963
best epoch:  47
best val loss:  18.755470055120963
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_47.params
current epoch:  48
validation batch 1 / 54, loss: 24.006275
val loss 19.028904667607062
current epoch:  49
validation batch 1 / 54, loss: 24.009829
val loss 18.746561571403785
best epoch:  49
best val loss:  18.746561571403785
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_49.params
global step: 8000, training loss: 13.558971, time: 0.301275s
current epoch:  50
validation batch 1 / 54, loss: 24.060177
val loss 18.888065408777308
current epoch:  51
validation batch 1 / 54, loss: 24.032858
val loss 18.749653551313614
current epoch:  52
validation batch 1 / 54, loss: 23.960520
val loss 18.767036464479233
current epoch:  53
validation batch 1 / 54, loss: 23.969181
val loss 18.82961181358055
current epoch:  54
validation batch 1 / 54, loss: 23.961578
val loss 18.767411920759415
current epoch:  55
validation batch 1 / 54, loss: 24.207888
val loss 18.73297965085065
best epoch:  55
best val loss:  18.73297965085065
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_55.params
current epoch:  56
validation batch 1 / 54, loss: 23.886995
val loss 18.876546603661996
global step: 9000, training loss: 18.087624, time: 1.297396s
current epoch:  57
validation batch 1 / 54, loss: 24.112944
val loss 18.681424291045577
best epoch:  57
best val loss:  18.681424291045577
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_57.params
current epoch:  58
validation batch 1 / 54, loss: 23.882023
val loss 18.849094682269627
current epoch:  59
validation batch 1 / 54, loss: 23.850956
val loss 18.733060563052142
current epoch:  60
validation batch 1 / 54, loss: 23.877661
val loss 18.660554753409492
best epoch:  60
best val loss:  18.660554753409492
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_60.params
current epoch:  61
validation batch 1 / 54, loss: 23.916986
val loss 18.624066591262817
best epoch:  61
best val loss:  18.624066591262817
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_61.params
current epoch:  62
validation batch 1 / 54, loss: 23.811371
val loss 18.697675122155083
global step: 10000, training loss: 16.719173, time: 1.297211s
current epoch:  63
validation batch 1 / 54, loss: 23.851160
val loss 18.952053997251724
current epoch:  64
validation batch 1 / 54, loss: 24.390804
val loss 18.70656970695213
current epoch:  65
validation batch 1 / 54, loss: 24.065388
val loss 18.62168033917745
best epoch:  65
best val loss:  18.62168033917745
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_65.params
current epoch:  66
validation batch 1 / 54, loss: 23.760515
val loss 18.65657890284503
current epoch:  67
validation batch 1 / 54, loss: 23.972527
val loss 18.614192653585363
best epoch:  67
best val loss:  18.614192653585363
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_67.params
current epoch:  68
validation batch 1 / 54, loss: 23.868624
val loss 18.58801371962936
best epoch:  68
best val loss:  18.58801371962936
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_68.params
global step: 11000, training loss: 17.193583, time: 1.298425s
current epoch:  69
validation batch 1 / 54, loss: 24.130018
val loss 18.638519357751917
current epoch:  70
validation batch 1 / 54, loss: 23.693239
val loss 18.64085566556012
current epoch:  71
validation batch 1 / 54, loss: 23.762243
val loss 18.620393929658114
current epoch:  72
validation batch 1 / 54, loss: 23.919292
val loss 18.61578673786587
current epoch:  73
validation batch 1 / 54, loss: 23.701464
val loss 18.630073105847394
current epoch:  74
validation batch 1 / 54, loss: 23.844864
val loss 18.636711924164384
global step: 12000, training loss: 9.987878, time: 0.395640s
current epoch:  75
validation batch 1 / 54, loss: 23.797146
val loss 18.679374447575324
current epoch:  76
validation batch 1 / 54, loss: 23.711107
val loss 18.54811524461817
best epoch:  76
best val loss:  18.54811524461817
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_76.params
current epoch:  77
validation batch 1 / 54, loss: 23.727457
val loss 18.69798187856321
current epoch:  78
validation batch 1 / 54, loss: 23.728989
val loss 18.666275174529463
current epoch:  79
validation batch 1 / 54, loss: 24.079929
val loss 18.563261146898622
current epoch:  80
validation batch 1 / 54, loss: 23.664717
val loss 18.68801290017587
current epoch:  81
validation batch 1 / 54, loss: 23.721685
val loss 18.75254542739303
global step: 13000, training loss: 16.262878, time: 1.316088s
current epoch:  82
validation batch 1 / 54, loss: 23.928770
val loss 18.607373396555584
current epoch:  83
validation batch 1 / 54, loss: 23.706524
val loss 18.680240781218917
current epoch:  84
validation batch 1 / 54, loss: 23.766945
val loss 18.58380369786863
current epoch:  85
validation batch 1 / 54, loss: 23.985802
val loss 18.576649224316633
current epoch:  86
validation batch 1 / 54, loss: 23.788660
val loss 18.51767905553182
best epoch:  86
best val loss:  18.51767905553182
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_86.params
current epoch:  87
validation batch 1 / 54, loss: 23.668613
val loss 18.666033806624235
global step: 14000, training loss: 16.247517, time: 1.296617s
current epoch:  88
validation batch 1 / 54, loss: 23.758368
val loss 18.587408516142105
current epoch:  89
validation batch 1 / 54, loss: 23.789875
val loss 18.555931992001003
current epoch:  90
validation batch 1 / 54, loss: 23.741604
val loss 18.63707473542955
current epoch:  91
validation batch 1 / 54, loss: 23.597519
val loss 18.632622506883408
current epoch:  92
validation batch 1 / 54, loss: 23.796307
val loss 18.567563233552157
current epoch:  93
validation batch 1 / 54, loss: 23.739202
val loss 18.729190632149024
global step: 15000, training loss: 15.538653, time: 1.287932s
current epoch:  94
validation batch 1 / 54, loss: 23.739119
val loss 18.56504100340384
current epoch:  95
validation batch 1 / 54, loss: 23.586201
val loss 18.568082924242372
current epoch:  96
validation batch 1 / 54, loss: 23.745655
val loss 18.584402172653764
current epoch:  97
validation batch 1 / 54, loss: 23.689444
val loss 18.594798105734366
current epoch:  98
validation batch 1 / 54, loss: 23.622829
val loss 18.684153389047694
current epoch:  99
validation batch 1 / 54, loss: 23.634750
val loss 18.595998304861563
global step: 16000, training loss: 18.465029, time: 0.303474s
current epoch:  100
validation batch 1 / 54, loss: 23.704449
val loss 18.58532268029672
current epoch:  101
validation batch 1 / 54, loss: 23.819902
val loss 18.58160768614875
current epoch:  102
validation batch 1 / 54, loss: 23.719006
val loss 18.549418246304548
current epoch:  103
validation batch 1 / 54, loss: 23.638174
val loss 18.6325872209337
current epoch:  104
validation batch 1 / 54, loss: 23.868401
val loss 18.527590195337932
current epoch:  105
validation batch 1 / 54, loss: 23.700050
val loss 18.549335303129972
current epoch:  106
validation batch 1 / 54, loss: 23.596731
val loss 18.60518385745861
global step: 17000, training loss: 16.888704, time: 1.311822s
current epoch:  107
validation batch 1 / 54, loss: 23.692234
val loss 18.619565177846837
current epoch:  108
validation batch 1 / 54, loss: 23.553883
val loss 18.62346969710456
current epoch:  109
validation batch 1 / 54, loss: 23.675804
val loss 18.59627232728181
best epoch: 86
Average Training Time: 80.5803 secs/epoch
Average Inference Time: 30.9008 secs
best epoch: 86
load weight from: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_86.params
predicting data set batch 1 / 54
input: (3394, 307, 1, 12)
prediction: (3394, 307, 12)
data_target_tensor: (3394, 307, 12)
current epoch: 86, predict 1-th point
MAE: 17.462906
RMSE: 28.099747
MAPE: 11.625860
current epoch: 86, predict 2-th point
MAE: 18.015724
RMSE: 29.099726
MAPE: 11.970004
current epoch: 86, predict 3-th point
MAE: 18.434374
RMSE: 29.849074
MAPE: 12.201048
current epoch: 86, predict 4-th point
MAE: 18.751289
RMSE: 30.459847
MAPE: 12.383883
current epoch: 86, predict 5-th point
MAE: 19.058556
RMSE: 31.038295
MAPE: 12.551454
current epoch: 86, predict 6-th point
MAE: 19.386572
RMSE: 31.603370
MAPE: 12.731037
current epoch: 86, predict 7-th point
MAE: 19.684771
RMSE: 32.115788
MAPE: 12.910482
current epoch: 86, predict 8-th point
MAE: 19.950247
RMSE: 32.564646
MAPE: 13.046791
current epoch: 86, predict 9-th point
MAE: 20.187874
RMSE: 32.973008
MAPE: 13.197458
current epoch: 86, predict 10-th point
MAE: 20.443687
RMSE: 33.371847
MAPE: 13.363774
current epoch: 86, predict 11-th point
MAE: 20.793213
RMSE: 33.866338
MAPE: 13.620567
current epoch: 86, predict 12-th point
MAE: 21.322210
RMSE: 34.564571
MAPE: 13.957171
all MAE: 19.457624
all RMSE: 31.690856
all MAPE: 12.796643
[17.462906, 28.09974669209949, 11.625859886407852, 18.015724, 29.099726242403253, 11.970003694295883, 18.434374, 29.849074091520723, 12.20104843378067, 18.75129, 30.45984745741361, 12.383883446455002, 19.058556, 31.038294612009132, 12.551453709602356, 19.386572, 31.603370235686988, 12.73103654384613, 19.68477, 32.115787832855666, 12.910482287406921, 19.950247, 32.564646117497, 13.046790659427643, 20.187874, 32.973007722700245, 13.197457790374756, 20.443687, 33.37184705378547, 13.363774120807648, 20.793213, 33.8663377560641, 13.620567321777344, 21.32221, 34.56457138062849, 13.957171142101288, 19.457624, 31.690856153937073, 12.796643376350403]

Process finished with exit code -1
