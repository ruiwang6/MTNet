ssh://root@region-42.seetacloud.com:47530/root/miniconda3/bin/python -u /project/STG-NCDE-main/model/Run_cde.py
/project/STG-NCDE-main
Namespace(batch_size=64, cheb_k=2, column_wise=False, comment='', cuda=True, dataset='PEMSD4', debug=False, default_graph=True, device=0, early_stop=True, early_stop_patience=15, embed_dim=10, epochs=100, g_type='agc', grad_norm=False, hid_dim=128, hid_hid_dim=128, horizon=12, input_dim=2, lag=12, log_dir='../runs', log_step=20, loss_func='mae', lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', lr_init=0.001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, missing_rate=0.1, missing_test=False, mode='train', model='GCDE', model_path='', model_type='type1', normalizer='std', num_layers=3, num_nodes=307, output_dim=1, plot=False, real_value=True, seed=10, solver='rk4', teacher_forcing=False, tensorboard=False, test_ratio=0.2, tod=False, val_ratio=0.2, weight_decay=0.001)
*****************Model Parameter*****************
node_embeddings torch.Size([307, 10]) True
func_f.linear_in.weight torch.Size([128, 128]) True
func_f.linear_in.bias torch.Size([128]) True
func_f.linears.0.weight torch.Size([128, 128]) True
func_f.linears.0.bias torch.Size([128]) True
func_f.linears.1.weight torch.Size([128, 128]) True
func_f.linears.1.bias torch.Size([128]) True
func_f.linear_out.weight torch.Size([256, 128]) True
func_f.linear_out.bias torch.Size([256]) True
func_g.node_embeddings torch.Size([307, 10]) True
func_g.weights_pool torch.Size([10, 2, 128, 128]) True
func_g.bias_pool torch.Size([10, 128]) True
func_g.linear_in.weight torch.Size([128, 128]) True
func_g.linear_in.bias torch.Size([128]) True
func_g.linear_out.weight torch.Size([16384, 128]) True
func_g.linear_out.bias torch.Size([16384]) True
end_conv.weight torch.Size([12, 1, 1, 128]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([128, 2]) True
initial_h.bias torch.Size([128]) True
initial_z.weight torch.Size([128, 2]) True
initial_z.bias torch.Size([128]) True
Total params num: 2550024
*****************Finish Parameter****************
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3
    (linear_in): Linear(in_features=128, out_features=128, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
    )
    (linear_out): Linear(in_features=128, out_features=256, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3
    (linear_in): Linear(in_features=128, out_features=128, bias=True)
    (linear_out): Linear(in_features=128, out_features=16384, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 128), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=128, bias=True)
  (initial_z): Linear(in_features=2, out_features=128, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([307, 10]) True
func_f.linear_in.weight torch.Size([128, 128]) True
func_f.linear_in.bias torch.Size([128]) True
func_f.linears.0.weight torch.Size([128, 128]) True
func_f.linears.0.bias torch.Size([128]) True
func_f.linears.1.weight torch.Size([128, 128]) True
func_f.linears.1.bias torch.Size([128]) True
func_f.linear_out.weight torch.Size([256, 128]) True
func_f.linear_out.bias torch.Size([256]) True
func_g.node_embeddings torch.Size([307, 10]) True
func_g.weights_pool torch.Size([10, 2, 128, 128]) True
func_g.bias_pool torch.Size([10, 128]) True
func_g.linear_in.weight torch.Size([128, 128]) True
func_g.linear_in.bias torch.Size([128]) True
func_g.linear_out.weight torch.Size([16384, 128]) True
func_g.linear_out.bias torch.Size([16384]) True
end_conv.weight torch.Size([12, 1, 1, 128]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([128, 2]) True
initial_h.bias torch.Size([128]) True
initial_z.weight torch.Size([128, 2]) True
initial_z.bias torch.Size([128]) True
Total params num: 2550024
*****************Finish Parameter****************
Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0
Normalize the dataset by Standard Normalization
Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)
Val:  (3375, 12, 307, 1) (3375, 12, 307, 1)
Test:  (3375, 12, 307, 1) (3375, 12, 307, 1)
Creat Log File in:  ../runs/PEMSD4/09-22-13h16m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}/run.log
2024-09-22 13:16: Experiment log path in: ../runs/PEMSD4/09-22-13h16m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}
*****************Model Parameter*****************
node_embeddings torch.Size([307, 10]) True
func_f.linear_in.weight torch.Size([128, 128]) True
func_f.linear_in.bias torch.Size([128]) True
func_f.linears.0.weight torch.Size([128, 128]) True
func_f.linears.0.bias torch.Size([128]) True
func_f.linears.1.weight torch.Size([128, 128]) True
func_f.linears.1.bias torch.Size([128]) True
func_f.linear_out.weight torch.Size([256, 128]) True
func_f.linear_out.bias torch.Size([256]) True
func_g.node_embeddings torch.Size([307, 10]) True
func_g.weights_pool torch.Size([10, 2, 128, 128]) True
func_g.bias_pool torch.Size([10, 128]) True
func_g.linear_in.weight torch.Size([128, 128]) True
func_g.linear_in.bias torch.Size([128]) True
func_g.linear_out.weight torch.Size([16384, 128]) True
func_g.linear_out.bias torch.Size([16384]) True
end_conv.weight torch.Size([12, 1, 1, 128]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([128, 2]) True
initial_h.bias torch.Size([128]) True
initial_z.weight torch.Size([128, 2]) True
initial_z.bias torch.Size([128]) True
Total params num: 2550024
*****************Finish Parameter****************
2024-09-22 13:16: Argument batch_size: 64
2024-09-22 13:16: Argument cheb_k: 2
2024-09-22 13:16: Argument column_wise: False
2024-09-22 13:16: Argument comment: ''
2024-09-22 13:16: Argument cuda: True
2024-09-22 13:16: Argument dataset: 'PEMSD4'
2024-09-22 13:16: Argument debug: False
2024-09-22 13:16: Argument default_graph: True
2024-09-22 13:16: Argument device: 0
2024-09-22 13:16: Argument early_stop: True
2024-09-22 13:16: Argument early_stop_patience: 15
2024-09-22 13:16: Argument embed_dim: 10
2024-09-22 13:16: Argument epochs: 100
2024-09-22 13:16: Argument g_type: 'agc'
2024-09-22 13:16: Argument grad_norm: False
2024-09-22 13:16: Argument hid_dim: 128
2024-09-22 13:16: Argument hid_hid_dim: 128
2024-09-22 13:16: Argument horizon: 12
2024-09-22 13:16: Argument input_dim: 2
2024-09-22 13:16: Argument lag: 12
2024-09-22 13:16: Argument log_dir: '../runs/PEMSD4/09-22-13h16m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}'
2024-09-22 13:16: Argument log_step: 20
2024-09-22 13:16: Argument loss_func: 'mae'
2024-09-22 13:16: Argument lr_decay: False
2024-09-22 13:16: Argument lr_decay_rate: 0.3
2024-09-22 13:16: Argument lr_decay_step: '5,20,40,70'
2024-09-22 13:16: Argument lr_init: 0.001
2024-09-22 13:16: Argument mae_thresh: None
2024-09-22 13:16: Argument mape_thresh: 0.0
2024-09-22 13:16: Argument max_grad_norm: 5
2024-09-22 13:16: Argument missing_rate: 0.1
2024-09-22 13:16: Argument missing_test: False
2024-09-22 13:16: Argument mode: 'train'
2024-09-22 13:16: Argument model: 'GCDE'
2024-09-22 13:16: Argument model_path: ''
2024-09-22 13:16: Argument model_type: 'type1'
2024-09-22 13:16: Argument normalizer: 'std'
2024-09-22 13:16: Argument num_layers: 3
2024-09-22 13:16: Argument num_nodes: 307
2024-09-22 13:16: Argument output_dim: 1
2024-09-22 13:16: Argument plot: False
2024-09-22 13:16: Argument real_value: True
2024-09-22 13:16: Argument seed: 10
2024-09-22 13:16: Argument solver: 'rk4'
2024-09-22 13:16: Argument teacher_forcing: False
2024-09-22 13:16: Argument tensorboard: False
2024-09-22 13:16: Argument test_ratio: 0.2
2024-09-22 13:16: Argument tod: False
2024-09-22 13:16: Argument val_ratio: 0.2
2024-09-22 13:16: Argument weight_decay: 0.001
2024-09-22 13:16: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3
    (linear_in): Linear(in_features=128, out_features=128, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
    )
    (linear_out): Linear(in_features=128, out_features=256, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3
    (linear_in): Linear(in_features=128, out_features=128, bias=True)
    (linear_out): Linear(in_features=128, out_features=16384, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 128), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=128, bias=True)
  (initial_z): Linear(in_features=2, out_features=128, bias=True)
)
2024-09-22 13:16: Total params: 2550024
2024-09-22 13:16: Train Epoch 1: 0/158 Loss: 187.581009
2024-09-22 13:17: Train Epoch 1: 20/158 Loss: 57.645531
2024-09-22 13:19: Train Epoch 1: 40/158 Loss: 39.106190
2024-09-22 13:20: Train Epoch 1: 60/158 Loss: 33.157146
2024-09-22 13:21: Train Epoch 1: 80/158 Loss: 32.433460
2024-09-22 13:22: Train Epoch 1: 100/158 Loss: 37.188297
2024-09-22 13:24: Train Epoch 1: 120/158 Loss: 29.766716
2024-09-22 13:25: Train Epoch 1: 140/158 Loss: 33.477589
2024-09-22 13:26: **********Train Epoch 1: averaged Loss: 47.156962
2024-09-22 13:27: **********Val Epoch 1: average Loss: 32.697017
2024-09-22 13:27: *********************************Current best model saved!
2024-09-22 13:27: Train Epoch 2: 0/158 Loss: 33.971172
2024-09-22 13:28: Train Epoch 2: 20/158 Loss: 38.650581
2024-09-22 13:29: Train Epoch 2: 40/158 Loss: 27.972923
2024-09-22 13:31: Train Epoch 2: 60/158 Loss: 25.544039
2024-09-22 13:32: Train Epoch 2: 80/158 Loss: 27.667038
2024-09-22 13:33: Train Epoch 2: 100/158 Loss: 26.787567
2024-09-22 13:34: Train Epoch 2: 120/158 Loss: 25.893200
2024-09-22 13:36: Train Epoch 2: 140/158 Loss: 26.228994
2024-09-22 13:37: **********Train Epoch 2: averaged Loss: 28.741751
2024-09-22 13:38: **********Val Epoch 2: average Loss: 26.530878
2024-09-22 13:38: *********************************Current best model saved!
2024-09-22 13:38: Train Epoch 3: 0/158 Loss: 25.242136
2024-09-22 13:39: Train Epoch 3: 20/158 Loss: 25.200975
2024-09-22 13:40: Train Epoch 3: 40/158 Loss: 24.608786
2024-09-22 13:42: Train Epoch 3: 60/158 Loss: 22.672373
2024-09-22 13:43: Train Epoch 3: 80/158 Loss: 27.430880
2024-09-22 13:44: Train Epoch 3: 100/158 Loss: 25.198296
2024-09-22 13:45: Train Epoch 3: 120/158 Loss: 26.050922
2024-09-22 13:47: Train Epoch 3: 140/158 Loss: 24.159615
2024-09-22 13:48: **********Train Epoch 3: averaged Loss: 24.932405
2024-09-22 13:49: **********Val Epoch 3: average Loss: 24.426348
2024-09-22 13:49: *********************************Current best model saved!
2024-09-22 13:49: Train Epoch 4: 0/158 Loss: 21.859364
2024-09-22 13:50: Train Epoch 4: 20/158 Loss: 24.353895
2024-09-22 13:51: Train Epoch 4: 40/158 Loss: 21.913826
2024-09-22 13:52: Train Epoch 4: 60/158 Loss: 23.363173
2024-09-22 13:54: Train Epoch 4: 80/158 Loss: 23.559019
2024-09-22 13:55: Train Epoch 4: 100/158 Loss: 22.850744
2024-09-22 13:56: Train Epoch 4: 120/158 Loss: 22.068121
2024-09-22 13:57: Train Epoch 4: 140/158 Loss: 22.089617
2024-09-22 13:58: **********Train Epoch 4: averaged Loss: 23.440473
2024-09-22 13:59: **********Val Epoch 4: average Loss: 23.476289
2024-09-22 13:59: *********************************Current best model saved!
2024-09-22 13:59: Train Epoch 5: 0/158 Loss: 22.082842
2024-09-22 14:01: Train Epoch 5: 20/158 Loss: 23.291885
2024-09-22 14:02: Train Epoch 5: 40/158 Loss: 23.832672
2024-09-22 14:03: Train Epoch 5: 60/158 Loss: 23.530693
2024-09-22 14:04: Train Epoch 5: 80/158 Loss: 23.502899
2024-09-22 14:06: Train Epoch 5: 100/158 Loss: 23.835714
2024-09-22 14:07: Train Epoch 5: 120/158 Loss: 22.016706
2024-09-22 14:08: Train Epoch 5: 140/158 Loss: 22.280413
2024-09-22 14:09: **********Train Epoch 5: averaged Loss: 22.480149
2024-09-22 14:10: **********Val Epoch 5: average Loss: 22.882354
2024-09-22 14:10: *********************************Current best model saved!
2024-09-22 14:10: Train Epoch 6: 0/158 Loss: 24.583826
2024-09-22 14:11: Train Epoch 6: 20/158 Loss: 21.293829
2024-09-22 14:13: Train Epoch 6: 40/158 Loss: 22.409266
2024-09-22 14:14: Train Epoch 6: 60/158 Loss: 22.458101
2024-09-22 14:15: Train Epoch 6: 80/158 Loss: 22.338320
2024-09-22 14:17: Train Epoch 6: 100/158 Loss: 22.489843
2024-09-22 14:18: Train Epoch 6: 120/158 Loss: 20.783464
2024-09-22 14:19: Train Epoch 6: 140/158 Loss: 22.122070
2024-09-22 14:20: **********Train Epoch 6: averaged Loss: 22.072980
2024-09-22 14:21: **********Val Epoch 6: average Loss: 22.658072
2024-09-22 14:21: *********************************Current best model saved!
2024-09-22 14:21: Train Epoch 7: 0/158 Loss: 22.400017
2024-09-22 14:22: Train Epoch 7: 20/158 Loss: 20.233879
2024-09-22 14:24: Train Epoch 7: 40/158 Loss: 21.186401
2024-09-22 14:25: Train Epoch 7: 60/158 Loss: 21.748835
2024-09-22 14:26: Train Epoch 7: 80/158 Loss: 21.566841
2024-09-22 14:27: Train Epoch 7: 100/158 Loss: 22.768311
2024-09-22 14:29: Train Epoch 7: 120/158 Loss: 20.283133
2024-09-22 14:30: Train Epoch 7: 140/158 Loss: 21.288074
2024-09-22 14:31: **********Train Epoch 7: averaged Loss: 21.274538
2024-09-22 14:32: **********Val Epoch 7: average Loss: 21.861533
2024-09-22 14:32: *********************************Current best model saved!
2024-09-22 14:32: Train Epoch 8: 0/158 Loss: 21.488461
2024-09-22 14:33: Train Epoch 8: 20/158 Loss: 21.949415
2024-09-22 14:34: Train Epoch 8: 40/158 Loss: 21.486814
2024-09-22 14:36: Train Epoch 8: 60/158 Loss: 20.902372
2024-09-22 14:37: Train Epoch 8: 80/158 Loss: 21.350588
2024-09-22 14:38: Train Epoch 8: 100/158 Loss: 21.414068
2024-09-22 14:39: Train Epoch 8: 120/158 Loss: 20.938875
2024-09-22 14:41: Train Epoch 8: 140/158 Loss: 23.364710
2024-09-22 14:42: **********Train Epoch 8: averaged Loss: 21.200772
2024-09-22 14:43: **********Val Epoch 8: average Loss: 22.543301
2024-09-22 14:43: Train Epoch 9: 0/158 Loss: 20.789080
2024-09-22 14:44: Train Epoch 9: 20/158 Loss: 20.001680
2024-09-22 14:45: Train Epoch 9: 40/158 Loss: 18.748974
2024-09-22 14:47: Train Epoch 9: 60/158 Loss: 21.331919
2024-09-22 14:48: Train Epoch 9: 80/158 Loss: 20.714144
2024-09-22 14:49: Train Epoch 9: 100/158 Loss: 20.775787
2024-09-22 14:50: Train Epoch 9: 120/158 Loss: 20.365751
2024-09-22 14:52: Train Epoch 9: 140/158 Loss: 20.467386
2024-09-22 14:53: **********Train Epoch 9: averaged Loss: 20.803754
2024-09-22 14:54: **********Val Epoch 9: average Loss: 21.382260
2024-09-22 14:54: *********************************Current best model saved!
2024-09-22 14:54: Train Epoch 10: 0/158 Loss: 21.229889
2024-09-22 14:55: Train Epoch 10: 20/158 Loss: 21.012054
2024-09-22 14:56: Train Epoch 10: 40/158 Loss: 20.590511
2024-09-22 14:57: Train Epoch 10: 60/158 Loss: 20.733437
2024-09-22 14:59: Train Epoch 10: 80/158 Loss: 19.583925
2024-09-22 15:00: Train Epoch 10: 100/158 Loss: 19.326107
2024-09-22 15:01: Train Epoch 10: 120/158 Loss: 21.739983
2024-09-22 15:02: Train Epoch 10: 140/158 Loss: 20.589882
2024-09-22 15:03: **********Train Epoch 10: averaged Loss: 20.454238
2024-09-22 15:04: **********Val Epoch 10: average Loss: 21.284081
2024-09-22 15:04: *********************************Current best model saved!
2024-09-22 15:04: Train Epoch 11: 0/158 Loss: 19.591690
2024-09-22 15:06: Train Epoch 11: 20/158 Loss: 20.324770
2024-09-22 15:07: Train Epoch 11: 40/158 Loss: 17.002066
2024-09-22 15:08: Train Epoch 11: 60/158 Loss: 20.851440
2024-09-22 15:09: Train Epoch 11: 80/158 Loss: 21.039761
2024-09-22 15:11: Train Epoch 11: 100/158 Loss: 18.999872
2024-09-22 15:12: Train Epoch 11: 120/158 Loss: 20.143595
2024-09-22 15:13: Train Epoch 11: 140/158 Loss: 21.050333
2024-09-22 15:14: **********Train Epoch 11: averaged Loss: 20.177885
2024-09-22 15:15: **********Val Epoch 11: average Loss: 21.241197
2024-09-22 15:15: *********************************Current best model saved!
2024-09-22 15:15: Train Epoch 12: 0/158 Loss: 19.694061
2024-09-22 15:16: Train Epoch 12: 20/158 Loss: 19.242493
2024-09-22 15:18: Train Epoch 12: 40/158 Loss: 18.832914
2024-09-22 15:19: Train Epoch 12: 60/158 Loss: 21.584528
2024-09-22 15:20: Train Epoch 12: 80/158 Loss: 20.840218
2024-09-22 15:21: Train Epoch 12: 100/158 Loss: 19.335363
2024-09-22 15:23: Train Epoch 12: 120/158 Loss: 19.549450
2024-09-22 15:24: Train Epoch 12: 140/158 Loss: 20.192066
2024-09-22 15:25: **********Train Epoch 12: averaged Loss: 19.956769
2024-09-22 15:26: **********Val Epoch 12: average Loss: 20.971242
2024-09-22 15:26: *********************************Current best model saved!
2024-09-22 15:26: Train Epoch 13: 0/158 Loss: 19.389496
2024-09-22 15:27: Train Epoch 13: 20/158 Loss: 20.103748
2024-09-22 15:29: Train Epoch 13: 40/158 Loss: 20.609865
2024-09-22 15:30: Train Epoch 13: 60/158 Loss: 17.446117
2024-09-22 15:31: Train Epoch 13: 80/158 Loss: 21.518917
2024-09-22 15:32: Train Epoch 13: 100/158 Loss: 19.039049
2024-09-22 15:34: Train Epoch 13: 120/158 Loss: 21.082722
2024-09-22 15:35: Train Epoch 13: 140/158 Loss: 20.085104
2024-09-22 15:36: **********Train Epoch 13: averaged Loss: 19.625216
2024-09-22 15:37: **********Val Epoch 13: average Loss: 20.982510
2024-09-22 15:37: Train Epoch 14: 0/158 Loss: 19.161568
2024-09-22 15:38: Train Epoch 14: 20/158 Loss: 20.823351
2024-09-22 15:39: Train Epoch 14: 40/158 Loss: 19.508928
2024-09-22 15:41: Train Epoch 14: 60/158 Loss: 21.793159
2024-09-22 15:42: Train Epoch 14: 80/158 Loss: 20.201828
2024-09-22 15:43: Train Epoch 14: 100/158 Loss: 20.077126
2024-09-22 15:44: Train Epoch 14: 120/158 Loss: 19.791666
2024-09-22 15:46: Train Epoch 14: 140/158 Loss: 19.539675
2024-09-22 15:47: **********Train Epoch 14: averaged Loss: 19.754555
2024-09-22 15:48: **********Val Epoch 14: average Loss: 20.598002
2024-09-22 15:48: *********************************Current best model saved!
2024-09-22 15:48: Train Epoch 15: 0/158 Loss: 20.740065
2024-09-22 15:49: Train Epoch 15: 20/158 Loss: 18.814579
2024-09-22 15:50: Train Epoch 15: 40/158 Loss: 17.316792
2024-09-22 15:52: Train Epoch 15: 60/158 Loss: 20.432981
2024-09-22 15:53: Train Epoch 15: 80/158 Loss: 20.029320
2024-09-22 15:54: Train Epoch 15: 100/158 Loss: 18.459658
2024-09-22 15:55: Train Epoch 15: 120/158 Loss: 18.574871
2024-09-22 15:57: Train Epoch 15: 140/158 Loss: 20.195309
2024-09-22 15:58: **********Train Epoch 15: averaged Loss: 19.479082
2024-09-22 15:59: **********Val Epoch 15: average Loss: 20.623610
2024-09-22 15:59: Train Epoch 16: 0/158 Loss: 17.264523
2024-09-22 16:00: Train Epoch 16: 20/158 Loss: 19.361759
2024-09-22 16:01: Train Epoch 16: 40/158 Loss: 20.141029
2024-09-22 16:02: Train Epoch 16: 60/158 Loss: 19.861887
2024-09-22 16:04: Train Epoch 16: 80/158 Loss: 20.719116
2024-09-22 16:05: Train Epoch 16: 100/158 Loss: 20.701849
2024-09-22 16:06: Train Epoch 16: 120/158 Loss: 19.142630
2024-09-22 16:07: Train Epoch 16: 140/158 Loss: 19.963226
2024-09-22 16:08: **********Train Epoch 16: averaged Loss: 19.334029
2024-09-22 16:09: **********Val Epoch 16: average Loss: 20.307848
2024-09-22 16:09: *********************************Current best model saved!
2024-09-22 16:09: Train Epoch 17: 0/158 Loss: 20.506245
2024-09-22 16:11: Train Epoch 17: 20/158 Loss: 19.211178
2024-09-22 16:12: Train Epoch 17: 40/158 Loss: 20.337637
2024-09-22 16:13: Train Epoch 17: 60/158 Loss: 18.324842
2024-09-22 16:14: Train Epoch 17: 80/158 Loss: 19.507666
2024-09-22 16:16: Train Epoch 17: 100/158 Loss: 18.266104
2024-09-22 16:17: Train Epoch 17: 120/158 Loss: 17.877787
2024-09-22 16:18: Train Epoch 17: 140/158 Loss: 19.815163
2024-09-22 16:19: **********Train Epoch 17: averaged Loss: 19.305500
2024-09-22 16:20: **********Val Epoch 17: average Loss: 21.548061
2024-09-22 16:20: Train Epoch 18: 0/158 Loss: 19.062214
2024-09-22 16:22: Train Epoch 18: 20/158 Loss: 19.143171
2024-09-22 16:23: Train Epoch 18: 40/158 Loss: 19.958176
2024-09-22 16:24: Train Epoch 18: 60/158 Loss: 20.839352
2024-09-22 16:25: Train Epoch 18: 80/158 Loss: 18.991890
2024-09-22 16:27: Train Epoch 18: 100/158 Loss: 18.902430
2024-09-22 16:28: Train Epoch 18: 120/158 Loss: 18.059036
2024-09-22 16:29: Train Epoch 18: 140/158 Loss: 18.322390
2024-09-22 16:30: **********Train Epoch 18: averaged Loss: 19.642115
2024-09-22 16:31: **********Val Epoch 18: average Loss: 20.285229
2024-09-22 16:31: *********************************Current best model saved!
2024-09-22 16:31: Train Epoch 19: 0/158 Loss: 19.664291
2024-09-22 16:32: Train Epoch 19: 20/158 Loss: 19.959824
2024-09-22 16:34: Train Epoch 19: 40/158 Loss: 19.616810
2024-09-22 16:35: Train Epoch 19: 60/158 Loss: 18.661015
2024-09-22 16:36: Train Epoch 19: 80/158 Loss: 20.659998
2024-09-22 16:37: Train Epoch 19: 100/158 Loss: 16.911249
2024-09-22 16:39: Train Epoch 19: 120/158 Loss: 18.430140
2024-09-22 16:40: Train Epoch 19: 140/158 Loss: 18.988604
2024-09-22 16:41: **********Train Epoch 19: averaged Loss: 18.969910
2024-09-22 16:42: **********Val Epoch 19: average Loss: 21.006224
2024-09-22 16:42: Train Epoch 20: 0/158 Loss: 20.073378
2024-09-22 16:43: Train Epoch 20: 20/158 Loss: 19.498970
2024-09-22 16:44: Train Epoch 20: 40/158 Loss: 19.668877
2024-09-22 16:46: Train Epoch 20: 60/158 Loss: 18.992693
2024-09-22 16:47: Train Epoch 20: 80/158 Loss: 18.753244
2024-09-22 16:48: Train Epoch 20: 100/158 Loss: 19.040396
2024-09-22 16:50: Train Epoch 20: 120/158 Loss: 20.592936
2024-09-22 16:51: Train Epoch 20: 140/158 Loss: 19.153938
2024-09-22 16:52: **********Train Epoch 20: averaged Loss: 19.560442
2024-09-22 16:53: **********Val Epoch 20: average Loss: 20.320110
2024-09-22 16:53: Train Epoch 21: 0/158 Loss: 18.009684
2024-09-22 16:54: Train Epoch 21: 20/158 Loss: 19.774214
2024-09-22 16:55: Train Epoch 21: 40/158 Loss: 17.790749
2024-09-22 16:57: Train Epoch 21: 60/158 Loss: 19.292604
2024-09-22 16:58: Train Epoch 21: 80/158 Loss: 19.049585
2024-09-22 16:59: Train Epoch 21: 100/158 Loss: 20.176571
2024-09-22 17:00: Train Epoch 21: 120/158 Loss: 18.750835
2024-09-22 17:02: Train Epoch 21: 140/158 Loss: 17.959227
2024-09-22 17:03: **********Train Epoch 21: averaged Loss: 18.890917
2024-09-22 17:04: **********Val Epoch 21: average Loss: 20.474091
2024-09-22 17:04: Train Epoch 22: 0/158 Loss: 17.529543
2024-09-22 17:05: Train Epoch 22: 20/158 Loss: 19.268011
2024-09-22 17:06: Train Epoch 22: 40/158 Loss: 17.724895
2024-09-22 17:07: Train Epoch 22: 60/158 Loss: 20.353359
2024-09-22 17:09: Train Epoch 22: 80/158 Loss: 19.148880
2024-09-22 17:10: Train Epoch 22: 100/158 Loss: 20.148676
2024-09-22 17:11: Train Epoch 22: 120/158 Loss: 20.233776
2024-09-22 17:12: Train Epoch 22: 140/158 Loss: 25.989708
2024-09-22 17:14: **********Train Epoch 22: averaged Loss: 20.051068
2024-09-22 17:14: **********Val Epoch 22: average Loss: 23.258426
2024-09-22 17:15: Train Epoch 23: 0/158 Loss: 21.096325
2024-09-22 17:16: Train Epoch 23: 20/158 Loss: 19.703558
2024-09-22 17:17: Train Epoch 23: 40/158 Loss: 18.814852
2024-09-22 17:18: Train Epoch 23: 60/158 Loss: 18.846954
2024-09-22 17:20: Train Epoch 23: 80/158 Loss: 19.994465
2024-09-22 17:21: Train Epoch 23: 100/158 Loss: 17.039623
2024-09-22 17:22: Train Epoch 23: 120/158 Loss: 18.007135
2024-09-22 17:23: Train Epoch 23: 140/158 Loss: 20.001547
2024-09-22 17:24: **********Train Epoch 23: averaged Loss: 19.604120
2024-09-22 17:25: **********Val Epoch 23: average Loss: 20.174615
2024-09-22 17:25: *********************************Current best model saved!
2024-09-22 17:25: Train Epoch 24: 0/158 Loss: 19.246790
2024-09-22 17:27: Train Epoch 24: 20/158 Loss: 19.910313
2024-09-22 17:28: Train Epoch 24: 40/158 Loss: 17.812822
2024-09-22 17:29: Train Epoch 24: 60/158 Loss: 20.024248
2024-09-22 17:30: Train Epoch 24: 80/158 Loss: 21.618553
2024-09-22 17:32: Train Epoch 24: 100/158 Loss: 20.417303
2024-09-22 17:33: Train Epoch 24: 120/158 Loss: 17.736889
2024-09-22 17:34: Train Epoch 24: 140/158 Loss: 18.521496
2024-09-22 17:35: **********Train Epoch 24: averaged Loss: 18.929093
2024-09-22 17:36: **********Val Epoch 24: average Loss: 20.494139
2024-09-22 17:36: Train Epoch 25: 0/158 Loss: 19.420568
2024-09-22 17:37: Train Epoch 25: 20/158 Loss: 18.353468
2024-09-22 17:39: Train Epoch 25: 40/158 Loss: 17.746689
2024-09-22 17:40: Train Epoch 25: 60/158 Loss: 19.211266
2024-09-22 17:41: Train Epoch 25: 80/158 Loss: 19.005608
2024-09-22 17:42: Train Epoch 25: 100/158 Loss: 18.898634
2024-09-22 17:44: Train Epoch 25: 120/158 Loss: 35.068233
2024-09-22 17:45: Train Epoch 25: 140/158 Loss: 24.424421
2024-09-22 17:46: **********Train Epoch 25: averaged Loss: 21.667397
2024-09-22 17:47: **********Val Epoch 25: average Loss: 22.875988
2024-09-22 17:47: Train Epoch 26: 0/158 Loss: 20.749514
2024-09-22 17:48: Train Epoch 26: 20/158 Loss: 20.010036
2024-09-22 17:50: Train Epoch 26: 40/158 Loss: 20.195072
2024-09-22 17:51: Train Epoch 26: 60/158 Loss: 19.694071
2024-09-22 17:52: Train Epoch 26: 80/158 Loss: 23.233757
2024-09-22 17:53: Train Epoch 26: 100/158 Loss: 18.368986
2024-09-22 17:55: Train Epoch 26: 120/158 Loss: 18.513521
2024-09-22 17:56: Train Epoch 26: 140/158 Loss: 20.317434
2024-09-22 17:57: **********Train Epoch 26: averaged Loss: 19.963330
2024-09-22 17:58: **********Val Epoch 26: average Loss: 20.792857
2024-09-22 17:58: Train Epoch 27: 0/158 Loss: 19.365973
2024-09-22 17:59: Train Epoch 27: 20/158 Loss: 17.707123
2024-09-22 18:00: Train Epoch 27: 40/158 Loss: 19.482624
2024-09-22 18:02: Train Epoch 27: 60/158 Loss: 21.118814
2024-09-22 18:03: Train Epoch 27: 80/158 Loss: 19.329067
2024-09-22 18:04: Train Epoch 27: 100/158 Loss: 19.246708
2024-09-22 18:05: Train Epoch 27: 120/158 Loss: 18.703304
2024-09-22 18:07: Train Epoch 27: 140/158 Loss: 42.816128
2024-09-22 18:08: **********Train Epoch 27: averaged Loss: 22.999786
2024-09-22 18:09: **********Val Epoch 27: average Loss: 27.089460
2024-09-22 18:09: Train Epoch 28: 0/158 Loss: 26.688400
2024-09-22 18:10: Train Epoch 28: 20/158 Loss: 23.101839
2024-09-22 18:11: Train Epoch 28: 40/158 Loss: 22.516907
2024-09-22 18:12: Train Epoch 28: 60/158 Loss: 20.629377
2024-09-22 18:14: Train Epoch 28: 80/158 Loss: 21.443409
2024-09-22 18:15: Train Epoch 28: 100/158 Loss: 18.453770
2024-09-22 18:16: Train Epoch 28: 120/158 Loss: 21.122795
2024-09-22 18:17: Train Epoch 28: 140/158 Loss: 21.050714
2024-09-22 18:18: **********Train Epoch 28: averaged Loss: 21.861033
2024-09-22 18:19: **********Val Epoch 28: average Loss: 22.194888
2024-09-22 18:20: Train Epoch 29: 0/158 Loss: 21.901989
2024-09-22 18:21: Train Epoch 29: 20/158 Loss: 19.665152
2024-09-22 18:22: Train Epoch 29: 40/158 Loss: 21.659389
2024-09-22 18:23: Train Epoch 29: 60/158 Loss: 19.136927
2024-09-22 18:25: Train Epoch 29: 80/158 Loss: 19.886568
2024-09-22 18:26: Train Epoch 29: 100/158 Loss: 20.002970
2024-09-22 18:27: Train Epoch 29: 120/158 Loss: 21.132288
2024-09-22 18:28: Train Epoch 29: 140/158 Loss: 21.499996
2024-09-22 18:29: **********Train Epoch 29: averaged Loss: 23.958228
2024-09-22 18:30: **********Val Epoch 29: average Loss: 40.670540
2024-09-22 18:30: Train Epoch 30: 0/158 Loss: 40.234444
2024-09-22 18:32: Train Epoch 30: 20/158 Loss: 26.789003
2024-09-22 18:33: Train Epoch 30: 40/158 Loss: 26.353935
2024-09-22 18:34: Train Epoch 30: 60/158 Loss: 24.832523
2024-09-22 18:35: Train Epoch 30: 80/158 Loss: 23.166986
2024-09-22 18:37: Train Epoch 30: 100/158 Loss: 20.775541
2024-09-22 18:38: Train Epoch 30: 120/158 Loss: 23.610147
2024-09-22 18:39: Train Epoch 30: 140/158 Loss: 22.175701
2024-09-22 18:40: **********Train Epoch 30: averaged Loss: 25.120419
2024-09-22 18:41: **********Val Epoch 30: average Loss: 23.336277
2024-09-22 18:41: Train Epoch 31: 0/158 Loss: 21.702375
2024-09-22 18:42: Train Epoch 31: 20/158 Loss: 22.237377
2024-09-22 18:44: Train Epoch 31: 40/158 Loss: 21.656595
2024-09-22 18:45: Train Epoch 31: 60/158 Loss: 20.748541
2024-09-22 18:46: Train Epoch 31: 80/158 Loss: 20.540831
2024-09-22 18:47: Train Epoch 31: 100/158 Loss: 20.599131
2024-09-22 18:49: Train Epoch 31: 120/158 Loss: 22.177212
2024-09-22 18:50: Train Epoch 31: 140/158 Loss: 22.451115
2024-09-22 18:51: **********Train Epoch 31: averaged Loss: 21.407556
2024-09-22 18:52: **********Val Epoch 31: average Loss: 22.521571
2024-09-22 18:52: Train Epoch 32: 0/158 Loss: 20.448088
2024-09-22 18:53: Train Epoch 32: 20/158 Loss: 22.233627
2024-09-22 18:55: Train Epoch 32: 40/158 Loss: 19.113388
2024-09-22 18:56: Train Epoch 32: 60/158 Loss: 21.313057
2024-09-22 18:57: Train Epoch 32: 80/158 Loss: 21.192785
2024-09-22 18:58: Train Epoch 32: 100/158 Loss: 19.606899
2024-09-22 19:00: Train Epoch 32: 120/158 Loss: 21.128992
2024-09-22 19:01: Train Epoch 32: 140/158 Loss: 20.065615
2024-09-22 19:02: **********Train Epoch 32: averaged Loss: 20.506003
2024-09-22 19:03: **********Val Epoch 32: average Loss: 21.255748
2024-09-22 19:03: Train Epoch 33: 0/158 Loss: 18.395697
2024-09-22 19:04: Train Epoch 33: 20/158 Loss: 19.356546
2024-09-22 19:05: Train Epoch 33: 40/158 Loss: 22.200827
2024-09-22 19:07: Train Epoch 33: 60/158 Loss: 20.189558
2024-09-22 19:08: Train Epoch 33: 80/158 Loss: 20.935457
2024-09-22 19:09: Train Epoch 33: 100/158 Loss: 20.133501
2024-09-22 19:10: Train Epoch 33: 120/158 Loss: 19.938932
2024-09-22 19:12: Train Epoch 33: 140/158 Loss: 19.714336
2024-09-22 19:13: **********Train Epoch 33: averaged Loss: 20.278791
2024-09-22 19:14: **********Val Epoch 33: average Loss: 21.665143
2024-09-22 19:14: Train Epoch 34: 0/158 Loss: 21.409445
2024-09-22 19:15: Train Epoch 34: 20/158 Loss: 21.323938
2024-09-22 19:16: Train Epoch 34: 40/158 Loss: 21.152290
2024-09-22 19:17: Train Epoch 34: 60/158 Loss: 19.788845
2024-09-22 19:19: Train Epoch 34: 80/158 Loss: 21.316010
2024-09-22 19:20: Train Epoch 34: 100/158 Loss: 19.469564
2024-09-22 19:21: Train Epoch 34: 120/158 Loss: 21.634960
2024-09-22 19:22: Train Epoch 34: 140/158 Loss: 20.167164
2024-09-22 19:23: **********Train Epoch 34: averaged Loss: 20.027271
2024-09-22 19:24: **********Val Epoch 34: average Loss: 20.975541
2024-09-22 19:24: Train Epoch 35: 0/158 Loss: 21.461430
2024-09-22 19:26: Train Epoch 35: 20/158 Loss: 19.422174
2024-09-22 19:27: Train Epoch 35: 40/158 Loss: 20.028378
2024-09-22 19:28: Train Epoch 35: 60/158 Loss: 20.869907
2024-09-22 19:30: Train Epoch 35: 80/158 Loss: 21.069252
2024-09-22 19:31: Train Epoch 35: 100/158 Loss: 22.072016
2024-09-22 19:32: Train Epoch 35: 120/158 Loss: 54.094734
2024-09-22 19:33: Train Epoch 35: 140/158 Loss: 62.922001
2024-09-22 19:34: **********Train Epoch 35: averaged Loss: 29.924662
2024-09-22 19:35: **********Val Epoch 35: average Loss: 29.436976
2024-09-22 19:35: Train Epoch 36: 0/158 Loss: 26.820818
2024-09-22 19:37: Train Epoch 36: 20/158 Loss: 35.441322
2024-09-22 19:38: Train Epoch 36: 40/158 Loss: 28.060345
2024-09-22 19:39: Train Epoch 36: 60/158 Loss: 22.027655
2024-09-22 19:40: Train Epoch 36: 80/158 Loss: 24.831833
2024-09-22 19:42: Train Epoch 36: 100/158 Loss: 23.489180
2024-09-22 19:43: Train Epoch 36: 120/158 Loss: 26.205452
2024-09-22 19:44: Train Epoch 36: 140/158 Loss: 22.769558
2024-09-22 19:45: **********Train Epoch 36: averaged Loss: 26.122264
2024-09-22 19:46: **********Val Epoch 36: average Loss: 24.553865
2024-09-22 19:46: Train Epoch 37: 0/158 Loss: 23.046200
2024-09-22 19:47: Train Epoch 37: 20/158 Loss: 22.569088
2024-09-22 19:49: Train Epoch 37: 40/158 Loss: 22.957537
2024-09-22 19:50: Train Epoch 37: 60/158 Loss: 20.952358
2024-09-22 19:51: Train Epoch 37: 80/158 Loss: 21.457644
2024-09-22 19:52: Train Epoch 37: 100/158 Loss: 21.197447
2024-09-22 19:54: Train Epoch 37: 120/158 Loss: 20.386787
2024-09-22 19:55: Train Epoch 37: 140/158 Loss: 23.542889
2024-09-22 19:56: **********Train Epoch 37: averaged Loss: 21.489867
2024-09-22 19:57: **********Val Epoch 37: average Loss: 22.237837
2024-09-22 19:57: Train Epoch 38: 0/158 Loss: 20.161243
2024-09-22 19:58: Train Epoch 38: 20/158 Loss: 21.671274
2024-09-22 19:59: Train Epoch 38: 40/158 Loss: 20.580353
2024-09-22 20:01: Train Epoch 38: 60/158 Loss: 21.535358
2024-09-22 20:02: Train Epoch 38: 80/158 Loss: 20.899027
2024-09-22 20:03: Train Epoch 38: 100/158 Loss: 20.038095
2024-09-22 20:04: Train Epoch 38: 120/158 Loss: 20.537128
2024-09-22 20:06: Train Epoch 38: 140/158 Loss: 19.524111
2024-09-22 20:07: **********Train Epoch 38: averaged Loss: 20.626909
2024-09-22 20:08: **********Val Epoch 38: average Loss: 21.639944
2024-09-22 20:08: Validation performance didn't improve for 15 epochs. Training stops.
2024-09-22 20:08: Total training time: 411.6749min, best loss: 20.174615
2024-09-22 20:08: Average Training Time: 171.2465 secs/epoch
2024-09-22 20:08: Average Inference Time: 56.2942 secs
2024-09-22 20:08: Saving current best model to ../runs/PEMSD4/09-22-13h16m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}/best_model.pth
Average Inference Time: 57.1469 secs
2024-09-22 20:09: Horizon 01, MAE: 17.48, RMSE: 27.88, MAPE: 12.1209%
2024-09-22 20:09: Horizon 02, MAE: 18.28, RMSE: 29.14, MAPE: 12.6178%
2024-09-22 20:09: Horizon 03, MAE: 18.89, RMSE: 30.10, MAPE: 12.9095%
2024-09-22 20:09: Horizon 04, MAE: 19.38, RMSE: 30.85, MAPE: 13.1229%
2024-09-22 20:09: Horizon 05, MAE: 19.74, RMSE: 31.42, MAPE: 13.2348%
2024-09-22 20:09: Horizon 06, MAE: 19.98, RMSE: 31.82, MAPE: 13.4146%
2024-09-22 20:09: Horizon 07, MAE: 20.27, RMSE: 32.25, MAPE: 13.6558%
2024-09-22 20:09: Horizon 08, MAE: 20.59, RMSE: 32.68, MAPE: 13.8877%
2024-09-22 20:09: Horizon 09, MAE: 20.83, RMSE: 33.06, MAPE: 14.1560%
2024-09-22 20:09: Horizon 10, MAE: 21.07, RMSE: 33.41, MAPE: 14.3584%
2024-09-22 20:09: Horizon 11, MAE: 21.44, RMSE: 33.92, MAPE: 14.6866%
2024-09-22 20:09: Horizon 12, MAE: 22.02, RMSE: 34.66, MAPE: 14.9444%
2024-09-22 20:09: Average Horizon, MAE: 20.00, RMSE: 31.82, MAPE: 13.5925%

Process finished with exit code -1
