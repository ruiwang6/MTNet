ssh://root@region-42.seetacloud.com:17240/root/miniconda3/bin/python -u /project/ASTGCN-pytorch-master/train_ASTGCN_r.py
Read configuration file: configurations/Manchester_astgcn.conf
CUDA: True cuda:0
folder_dir: astgcn_r_h1d1w1_channel1_1.000000e-03
params_path: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03
load file: /project/ASTGCN-pytorch-master/data/Manchester/ManchesterDataFinall_r1_d1_w1_astcgn
train: torch.Size([7083, 277, 1, 4]) torch.Size([7083, 277, 1, 4]) torch.Size([7083, 277, 1, 4]) torch.Size([7083, 277, 4])
val: torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 4])
test: torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 1, 4]) torch.Size([2361, 277, 4])
(277, 277)
create params directory experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 2
nb_chev_filter	 64
nb_time_filter	 64
time_strides	 [1, 1, 1]
batch_size	 32
graph_signal_matrix_filename	 /project/ASTGCN-pytorch-master/data/Manchester/ManchesterDataFinall.npz
start_epoch	 0
epochs	 80
ASTGCN(
  (submodule): ModuleList(
    (0): ASTGCN_submodule(
      (BlockList): ModuleList(
        (0): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (final_conv): Conv2d(4, 4, kernel_size=(1, 64), stride=(1, 1))
    )
    (1): ASTGCN_submodule(
      (BlockList): ModuleList(
        (0): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (final_conv): Conv2d(4, 4, kernel_size=(1, 64), stride=(1, 1))
    )
    (2): ASTGCN_submodule(
      (BlockList): ModuleList(
        (0): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): ASTGCN_block(
          (TAt): Temporal_Attention_layer()
          (SAt): Spatial_Attention_layer()
          (cheb_conv_SAt): cheb_conv_withSAt(
            (Theta): ParameterList(
                (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
                (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            )
          )
          (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (final_conv): Conv2d(4, 4, kernel_size=(1, 64), stride=(1, 1))
    )
  )
)
Net's state_dict:
submodule.0.W 	 torch.Size([277, 4])
submodule.0.BlockList.0.TAt.U1 	 torch.Size([277])
submodule.0.BlockList.0.TAt.U2 	 torch.Size([1, 277])
submodule.0.BlockList.0.TAt.U3 	 torch.Size([1])
submodule.0.BlockList.0.TAt.be 	 torch.Size([1, 4, 4])
submodule.0.BlockList.0.TAt.Ve 	 torch.Size([4, 4])
submodule.0.BlockList.0.SAt.W1 	 torch.Size([4])
submodule.0.BlockList.0.SAt.W2 	 torch.Size([1, 4])
submodule.0.BlockList.0.SAt.W3 	 torch.Size([1])
submodule.0.BlockList.0.SAt.bs 	 torch.Size([1, 277, 277])
submodule.0.BlockList.0.SAt.Vs 	 torch.Size([277, 277])
submodule.0.BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 64])
submodule.0.BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 64])
submodule.0.BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 64])
submodule.0.BlockList.0.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.0.BlockList.0.time_conv.bias 	 torch.Size([64])
submodule.0.BlockList.0.residual_conv.weight 	 torch.Size([64, 1, 1, 1])
submodule.0.BlockList.0.residual_conv.bias 	 torch.Size([64])
submodule.0.BlockList.0.ln.weight 	 torch.Size([64])
submodule.0.BlockList.0.ln.bias 	 torch.Size([64])
submodule.0.BlockList.1.TAt.U1 	 torch.Size([277])
submodule.0.BlockList.1.TAt.U2 	 torch.Size([64, 277])
submodule.0.BlockList.1.TAt.U3 	 torch.Size([64])
submodule.0.BlockList.1.TAt.be 	 torch.Size([1, 4, 4])
submodule.0.BlockList.1.TAt.Ve 	 torch.Size([4, 4])
submodule.0.BlockList.1.SAt.W1 	 torch.Size([4])
submodule.0.BlockList.1.SAt.W2 	 torch.Size([64, 4])
submodule.0.BlockList.1.SAt.W3 	 torch.Size([64])
submodule.0.BlockList.1.SAt.bs 	 torch.Size([1, 277, 277])
submodule.0.BlockList.1.SAt.Vs 	 torch.Size([277, 277])
submodule.0.BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([64, 64])
submodule.0.BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([64, 64])
submodule.0.BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([64, 64])
submodule.0.BlockList.1.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.0.BlockList.1.time_conv.bias 	 torch.Size([64])
submodule.0.BlockList.1.residual_conv.weight 	 torch.Size([64, 64, 1, 1])
submodule.0.BlockList.1.residual_conv.bias 	 torch.Size([64])
submodule.0.BlockList.1.ln.weight 	 torch.Size([64])
submodule.0.BlockList.1.ln.bias 	 torch.Size([64])
submodule.0.final_conv.weight 	 torch.Size([4, 4, 1, 64])
submodule.0.final_conv.bias 	 torch.Size([4])
submodule.1.W 	 torch.Size([277, 4])
submodule.1.BlockList.0.TAt.U1 	 torch.Size([277])
submodule.1.BlockList.0.TAt.U2 	 torch.Size([1, 277])
submodule.1.BlockList.0.TAt.U3 	 torch.Size([1])
submodule.1.BlockList.0.TAt.be 	 torch.Size([1, 4, 4])
submodule.1.BlockList.0.TAt.Ve 	 torch.Size([4, 4])
submodule.1.BlockList.0.SAt.W1 	 torch.Size([4])
submodule.1.BlockList.0.SAt.W2 	 torch.Size([1, 4])
submodule.1.BlockList.0.SAt.W3 	 torch.Size([1])
submodule.1.BlockList.0.SAt.bs 	 torch.Size([1, 277, 277])
submodule.1.BlockList.0.SAt.Vs 	 torch.Size([277, 277])
submodule.1.BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 64])
submodule.1.BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 64])
submodule.1.BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 64])
submodule.1.BlockList.0.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.1.BlockList.0.time_conv.bias 	 torch.Size([64])
submodule.1.BlockList.0.residual_conv.weight 	 torch.Size([64, 1, 1, 1])
submodule.1.BlockList.0.residual_conv.bias 	 torch.Size([64])
submodule.1.BlockList.0.ln.weight 	 torch.Size([64])
submodule.1.BlockList.0.ln.bias 	 torch.Size([64])
submodule.1.BlockList.1.TAt.U1 	 torch.Size([277])
submodule.1.BlockList.1.TAt.U2 	 torch.Size([64, 277])
submodule.1.BlockList.1.TAt.U3 	 torch.Size([64])
submodule.1.BlockList.1.TAt.be 	 torch.Size([1, 4, 4])
submodule.1.BlockList.1.TAt.Ve 	 torch.Size([4, 4])
submodule.1.BlockList.1.SAt.W1 	 torch.Size([4])
submodule.1.BlockList.1.SAt.W2 	 torch.Size([64, 4])
submodule.1.BlockList.1.SAt.W3 	 torch.Size([64])
submodule.1.BlockList.1.SAt.bs 	 torch.Size([1, 277, 277])
submodule.1.BlockList.1.SAt.Vs 	 torch.Size([277, 277])
submodule.1.BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([64, 64])
submodule.1.BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([64, 64])
submodule.1.BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([64, 64])
submodule.1.BlockList.1.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.1.BlockList.1.time_conv.bias 	 torch.Size([64])
submodule.1.BlockList.1.residual_conv.weight 	 torch.Size([64, 64, 1, 1])
submodule.1.BlockList.1.residual_conv.bias 	 torch.Size([64])
submodule.1.BlockList.1.ln.weight 	 torch.Size([64])
submodule.1.BlockList.1.ln.bias 	 torch.Size([64])
submodule.1.final_conv.weight 	 torch.Size([4, 4, 1, 64])
submodule.1.final_conv.bias 	 torch.Size([4])
submodule.2.W 	 torch.Size([277, 4])
submodule.2.BlockList.0.TAt.U1 	 torch.Size([277])
submodule.2.BlockList.0.TAt.U2 	 torch.Size([1, 277])
submodule.2.BlockList.0.TAt.U3 	 torch.Size([1])
submodule.2.BlockList.0.TAt.be 	 torch.Size([1, 4, 4])
submodule.2.BlockList.0.TAt.Ve 	 torch.Size([4, 4])
submodule.2.BlockList.0.SAt.W1 	 torch.Size([4])
submodule.2.BlockList.0.SAt.W2 	 torch.Size([1, 4])
submodule.2.BlockList.0.SAt.W3 	 torch.Size([1])
submodule.2.BlockList.0.SAt.bs 	 torch.Size([1, 277, 277])
submodule.2.BlockList.0.SAt.Vs 	 torch.Size([277, 277])
submodule.2.BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 64])
submodule.2.BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 64])
submodule.2.BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 64])
submodule.2.BlockList.0.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.2.BlockList.0.time_conv.bias 	 torch.Size([64])
submodule.2.BlockList.0.residual_conv.weight 	 torch.Size([64, 1, 1, 1])
submodule.2.BlockList.0.residual_conv.bias 	 torch.Size([64])
submodule.2.BlockList.0.ln.weight 	 torch.Size([64])
submodule.2.BlockList.0.ln.bias 	 torch.Size([64])
submodule.2.BlockList.1.TAt.U1 	 torch.Size([277])
submodule.2.BlockList.1.TAt.U2 	 torch.Size([64, 277])
submodule.2.BlockList.1.TAt.U3 	 torch.Size([64])
submodule.2.BlockList.1.TAt.be 	 torch.Size([1, 4, 4])
submodule.2.BlockList.1.TAt.Ve 	 torch.Size([4, 4])
submodule.2.BlockList.1.SAt.W1 	 torch.Size([4])
submodule.2.BlockList.1.SAt.W2 	 torch.Size([64, 4])
submodule.2.BlockList.1.SAt.W3 	 torch.Size([64])
submodule.2.BlockList.1.SAt.bs 	 torch.Size([1, 277, 277])
submodule.2.BlockList.1.SAt.Vs 	 torch.Size([277, 277])
submodule.2.BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([64, 64])
submodule.2.BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([64, 64])
submodule.2.BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([64, 64])
submodule.2.BlockList.1.time_conv.weight 	 torch.Size([64, 64, 1, 3])
submodule.2.BlockList.1.time_conv.bias 	 torch.Size([64])
submodule.2.BlockList.1.residual_conv.weight 	 torch.Size([64, 64, 1, 1])
submodule.2.BlockList.1.residual_conv.bias 	 torch.Size([64])
submodule.2.BlockList.1.ln.weight 	 torch.Size([64])
submodule.2.BlockList.1.ln.bias 	 torch.Size([64])
submodule.2.final_conv.weight 	 torch.Size([4, 4, 1, 64])
submodule.2.final_conv.bias 	 torch.Size([4])
Net's total params: 1109403
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]}]
validation batch 1 / 74, loss: 8283.00
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_0.params
validation batch 1 / 74, loss: 408.65
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_1.params
validation batch 1 / 74, loss: 208.98
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_2.params
global step: 500, training loss: 91.46, time: 114.95s
validation batch 1 / 74, loss: 188.11
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_3.params
validation batch 1 / 74, loss: 193.44
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_4.params
global step: 1000, training loss: 66.39, time: 218.05s
validation batch 1 / 74, loss: 184.89
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_5.params
validation batch 1 / 74, loss: 184.90
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_6.params
global step: 1500, training loss: 51.83, time: 321.44s
validation batch 1 / 74, loss: 177.53
save parameters to file: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_7.params
validation batch 1 / 74, loss: 177.64
validation batch 1 / 74, loss: 178.11
global step: 2000, training loss: 50.67, time: 433.55s
validation batch 1 / 74, loss: 180.75
validation batch 1 / 74, loss: 177.78
global step: 2500, training loss: 49.80, time: 537.13s
validation batch 1 / 74, loss: 183.80
validation batch 1 / 74, loss: 175.66
global step: 3000, training loss: 42.33, time: 639.53s
validation batch 1 / 74, loss: 179.63
validation batch 1 / 74, loss: 181.69
global step: 3500, training loss: 38.92, time: 743.03s
validation batch 1 / 74, loss: 178.65
validation batch 1 / 74, loss: 183.35
validation batch 1 / 74, loss: 184.33
global step: 4000, training loss: 30.56, time: 856.84s
validation batch 1 / 74, loss: 182.00
validation batch 1 / 74, loss: 182.48
global step: 4500, training loss: 45.40, time: 960.34s
validation batch 1 / 74, loss: 191.78
validation batch 1 / 74, loss: 188.89
global step: 5000, training loss: 47.26, time: 1063.13s
validation batch 1 / 74, loss: 186.37
validation batch 1 / 74, loss: 185.73
global step: 5500, training loss: 40.48, time: 1164.75s
validation batch 1 / 74, loss: 192.19
validation batch 1 / 74, loss: 194.61
validation batch 1 / 74, loss: 200.60
global step: 6000, training loss: 36.75, time: 1280.05s
validation batch 1 / 74, loss: 192.04
validation batch 1 / 74, loss: 192.51
global step: 6500, training loss: 44.74, time: 1383.23s
validation batch 1 / 74, loss: 192.09
validation batch 1 / 74, loss: 198.64
global step: 7000, training loss: 31.60, time: 1487.64s
validation batch 1 / 74, loss: 200.00
validation batch 1 / 74, loss: 190.71
global step: 7500, training loss: 35.83, time: 1590.74s
validation batch 1 / 74, loss: 185.22
validation batch 1 / 74, loss: 191.79
validation batch 1 / 74, loss: 190.85
global step: 8000, training loss: 34.23, time: 1705.74s
validation batch 1 / 74, loss: 187.65
validation batch 1 / 74, loss: 198.28
global step: 8500, training loss: 38.94, time: 1808.03s
validation batch 1 / 74, loss: 196.36
validation batch 1 / 74, loss: 199.10
global step: 9000, training loss: 35.08, time: 1909.65s
validation batch 1 / 74, loss: 199.98
validation batch 1 / 74, loss: 196.29
global step: 9500, training loss: 36.53, time: 2012.54s
validation batch 1 / 74, loss: 194.04
validation batch 1 / 74, loss: 204.54
validation batch 1 / 74, loss: 208.89
global step: 10000, training loss: 33.97, time: 2127.73s
validation batch 1 / 74, loss: 211.96
validation batch 1 / 74, loss: 198.03
global step: 10500, training loss: 24.49, time: 2232.43s
validation batch 1 / 74, loss: 206.25
validation batch 1 / 74, loss: 206.89
global step: 11000, training loss: 25.34, time: 2336.43s
validation batch 1 / 74, loss: 206.76
validation batch 1 / 74, loss: 192.55
global step: 11500, training loss: 81.71, time: 2439.23s
validation batch 1 / 74, loss: 205.15
validation batch 1 / 74, loss: 202.73
validation batch 1 / 74, loss: 201.30
global step: 12000, training loss: 35.26, time: 2553.03s
validation batch 1 / 74, loss: 207.15
validation batch 1 / 74, loss: 199.39
global step: 12500, training loss: 27.89, time: 2657.83s
validation batch 1 / 74, loss: 201.99
validation batch 1 / 74, loss: 203.56
global step: 13000, training loss: 36.69, time: 2761.34s
validation batch 1 / 74, loss: 201.59
validation batch 1 / 74, loss: 198.55
global step: 13500, training loss: 32.94, time: 2867.33s
validation batch 1 / 74, loss: 205.36
validation batch 1 / 74, loss: 203.72
validation batch 1 / 74, loss: 207.23
global step: 14000, training loss: 34.61, time: 2982.65s
validation batch 1 / 74, loss: 201.85
validation batch 1 / 74, loss: 206.66
global step: 14500, training loss: 31.95, time: 3088.51s
validation batch 1 / 74, loss: 202.29
validation batch 1 / 74, loss: 205.04
global step: 15000, training loss: 21.03, time: 3190.91s
validation batch 1 / 74, loss: 201.78
validation batch 1 / 74, loss: 207.70
global step: 15500, training loss: 27.00, time: 3293.20s
validation batch 1 / 74, loss: 201.07
validation batch 1 / 74, loss: 208.77
validation batch 1 / 74, loss: 207.90
global step: 16000, training loss: 29.74, time: 3408.20s
validation batch 1 / 74, loss: 210.23
validation batch 1 / 74, loss: 204.40
global step: 16500, training loss: 34.32, time: 3510.52s
validation batch 1 / 74, loss: 207.71
validation batch 1 / 74, loss: 205.23
global step: 17000, training loss: 74.81, time: 3613.62s
validation batch 1 / 74, loss: 201.65
validation batch 1 / 74, loss: 204.08
global step: 17500, training loss: 29.38, time: 3716.00s
validation batch 1 / 74, loss: 208.32
best epoch: 7
Average Training Time: 36.0797 secs/epoch
Average Inference Time: 11.0535 secs
load weight from: experiments2/Manchester/astgcn_r_h1d1w1_channel1_1.000000e-03/epoch_7.params
predicting data set batch 1 / 74
prediction: (2361, 277, 4)
data_target_tensor: (2361, 277, 4)
current epoch: 7, predict 0 points
MAE: 3.75
RMSE: 8.16
MAPE: 5.72
current epoch: 7, predict 1 points
MAE: 4.25
RMSE: 9.27
MAPE: 6.56
current epoch: 7, predict 2 points
MAE: 4.59
RMSE: 9.97
MAPE: 7.15
current epoch: 7, predict 3 points
MAE: 4.97
RMSE: 10.65
MAPE: 7.78
all MAE: 4.39
all RMSE: 9.56
all MAPE: 6.81
[3.7491398, 8.15885562127607, 5.722750723361969, 4.2530613, 9.269481454643959, 6.5647415816783905, 4.5945272, 9.971240841585285, 7.152172923088074, 4.974174, 10.650576336806958, 7.784371078014374, 4.3927264, 9.55707351199987, 6.806002557277679]

Process finished with exit code -1
