ssh://root@region-42.seetacloud.com:52222/root/miniconda3/bin/python -u /project/DSTAGNN-main/train_DSTAGNN_my.py
Read configuration file: configurations/Manchester_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/Manchester/Manchester_r1_d0_w0_dstagnn
train: torch.Size([8693, 277, 1, 4]) torch.Size([8693, 277, 4])
val: torch.Size([2898, 277, 1, 4]) torch.Size([2898, 277, 4])
test: torch.Size([2898, 277, 1, 4]) torch.Size([2898, 277, 4])
delete the old one and create params directory myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 64
graph_signal_matrix_filename	 ./data/Manchester/Manchester.npz
start_epoch	 0
epochs	 100
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(16, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=4, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 4, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.0.EmbedT.norm.weight 	 torch.Size([277])
BlockList.0.EmbedT.norm.bias 	 torch.Size([277])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.0.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.0.fcmy.0.bias 	 torch.Size([4])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.1.EmbedT.norm.weight 	 torch.Size([277])
BlockList.1.EmbedT.norm.bias 	 torch.Size([277])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.1.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.1.fcmy.0.bias 	 torch.Size([4])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.2.EmbedT.norm.weight 	 torch.Size([277])
BlockList.2.EmbedT.norm.bias 	 torch.Size([277])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.2.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.2.fcmy.0.bias 	 torch.Size([4])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.3.EmbedT.norm.weight 	 torch.Size([277])
BlockList.3.EmbedT.norm.bias 	 torch.Size([277])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.3.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.3.fcmy.0.bias 	 torch.Size([4])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 16, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([4, 128])
final_fc.bias 	 torch.Size([4])
Net's total params: 2622040
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]}]
current epoch:  0
validation batch 1 / 46, loss: 329.60
val loss 406.1196975708008
best epoch:  0
best val loss:  406.1196975708008
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 46, loss: 292.77
val loss 367.7979673302692
best epoch:  1
best val loss:  367.7979673302692
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 46, loss: 241.09
val loss 306.3330884187118
best epoch:  2
best val loss:  306.3330884187118
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 46, loss: 201.96
val loss 252.27006381490955
best epoch:  3
best val loss:  252.27006381490955
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 46, loss: 143.66
val loss 181.3901080255923
best epoch:  4
best val loss:  181.3901080255923
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 46, loss: 103.33
val loss 124.73931926229726
best epoch:  5
best val loss:  124.73931926229726
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 46, loss: 76.18
val loss 89.3161451920219
best epoch:  6
best val loss:  89.3161451920219
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
current epoch:  7
validation batch 1 / 46, loss: 60.34
val loss 70.28493508048679
best epoch:  7
best val loss:  70.28493508048679
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
global step: 1000, training loss: 59.44, time: 582.98s
current epoch:  8
validation batch 1 / 46, loss: 54.19
val loss 62.494358892026156
best epoch:  8
best val loss:  62.494358892026156
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 46, loss: 49.33
val loss 56.65064343162205
best epoch:  9
best val loss:  56.65064343162205
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 46, loss: 45.09
val loss 51.621635354083516
best epoch:  10
best val loss:  51.621635354083516
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 46, loss: 42.10
val loss 48.34290160303531
best epoch:  11
best val loss:  48.34290160303531
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 46, loss: 41.31
val loss 47.623823539070465
best epoch:  12
best val loss:  47.623823539070465
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
current epoch:  13
validation batch 1 / 46, loss: 38.89
val loss 45.07676074815833
best epoch:  13
best val loss:  45.07676074815833
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_13.params
current epoch:  14
validation batch 1 / 46, loss: 38.84
val loss 45.255836362424105
global step: 2000, training loss: 44.10, time: 1157.89s
current epoch:  15
validation batch 1 / 46, loss: 36.62
val loss 43.18786762071692
best epoch:  15
best val loss:  43.18786762071692
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 46, loss: 36.83
val loss 43.1614002559496
best epoch:  16
best val loss:  43.1614002559496
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_16.params
current epoch:  17
validation batch 1 / 46, loss: 34.23
val loss 40.6884765210359
best epoch:  17
best val loss:  40.6884765210359
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
current epoch:  18
validation batch 1 / 46, loss: 34.95
val loss 41.50610285219939
current epoch:  19
validation batch 1 / 46, loss: 34.69
val loss 41.22328812143077
current epoch:  20
validation batch 1 / 46, loss: 34.93
val loss 41.59308470850406
current epoch:  21
validation batch 1 / 46, loss: 33.43
val loss 39.72270459714143
best epoch:  21
best val loss:  39.72270459714143
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_21.params
current epoch:  22
validation batch 1 / 46, loss: 32.70
val loss 39.1583660374517
best epoch:  22
best val loss:  39.1583660374517
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_22.params
global step: 3000, training loss: 37.55, time: 1761.88s
current epoch:  23
validation batch 1 / 46, loss: 34.12
val loss 41.17185605090597
current epoch:  24
validation batch 1 / 46, loss: 30.83
val loss 37.72248877649722
best epoch:  24
best val loss:  37.72248877649722
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
current epoch:  25
validation batch 1 / 46, loss: 31.69
val loss 38.162609141805895
current epoch:  26
validation batch 1 / 46, loss: 31.34
val loss 37.928125132685125
current epoch:  27
validation batch 1 / 46, loss: 31.14
val loss 38.02824215267015
current epoch:  28
validation batch 1 / 46, loss: 31.68
val loss 38.31073462444803
current epoch:  29
validation batch 1 / 46, loss: 31.47
val loss 38.08654320758322
global step: 4000, training loss: 37.23, time: 2386.18s
current epoch:  30
validation batch 1 / 46, loss: 30.58
val loss 37.586123756740406
best epoch:  30
best val loss:  37.586123756740406
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_30.params
current epoch:  31
validation batch 1 / 46, loss: 30.04
val loss 36.72899859884511
best epoch:  31
best val loss:  36.72899859884511
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_31.params
current epoch:  32
validation batch 1 / 46, loss: 30.01
val loss 37.42671896063763
current epoch:  33
validation batch 1 / 46, loss: 30.40
val loss 36.744896515556
current epoch:  34
validation batch 1 / 46, loss: 30.48
val loss 36.98335601972497
current epoch:  35
validation batch 1 / 46, loss: 29.52
val loss 35.77317080290421
best epoch:  35
best val loss:  35.77317080290421
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_35.params
current epoch:  36
validation batch 1 / 46, loss: 29.60
val loss 36.098704047825024
global step: 5000, training loss: 30.83, time: 2998.36s
current epoch:  37
validation batch 1 / 46, loss: 28.51
val loss 35.084107398986816
best epoch:  37
best val loss:  35.084107398986816
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_37.params
current epoch:  38
validation batch 1 / 46, loss: 28.86
val loss 35.538474000018574
current epoch:  39
validation batch 1 / 46, loss: 29.98
val loss 37.928659563479215
current epoch:  40
validation batch 1 / 46, loss: 29.21
val loss 36.51020477129065
current epoch:  41
validation batch 1 / 46, loss: 29.77
val loss 37.197414190872855
current epoch:  42
validation batch 1 / 46, loss: 29.79
val loss 35.61086708566417
current epoch:  43
validation batch 1 / 46, loss: 27.61
val loss 34.67575645446777
best epoch:  43
best val loss:  34.67575645446777
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_43.params
current epoch:  44
validation batch 1 / 46, loss: 28.78
val loss 35.48058356409488
global step: 6000, training loss: 30.78, time: 3635.88s
current epoch:  45
validation batch 1 / 46, loss: 27.96
val loss 34.71863779814347
current epoch:  46
validation batch 1 / 46, loss: 28.64
val loss 34.508062652919605
best epoch:  46
best val loss:  34.508062652919605
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_46.params
current epoch:  47
validation batch 1 / 46, loss: 27.74
val loss 34.25529098510742
best epoch:  47
best val loss:  34.25529098510742
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_47.params
current epoch:  48
validation batch 1 / 46, loss: 28.09
val loss 34.921720712081246
current epoch:  49
validation batch 1 / 46, loss: 28.09
val loss 34.47447105076002
current epoch:  50
validation batch 1 / 46, loss: 27.20
val loss 33.67331123352051
best epoch:  50
best val loss:  33.67331123352051
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_50.params
current epoch:  51
validation batch 1 / 46, loss: 27.35
val loss 33.56923443338145
best epoch:  51
best val loss:  33.56923443338145
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_51.params
global step: 7000, training loss: 27.32, time: 4259.66s
current epoch:  52
validation batch 1 / 46, loss: 27.46
val loss 33.795130439426586
current epoch:  53
validation batch 1 / 46, loss: 26.76
val loss 33.71990448495616
current epoch:  54
validation batch 1 / 46, loss: 26.29
val loss 32.93585126296334
best epoch:  54
best val loss:  32.93585126296334
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_54.params
current epoch:  55
validation batch 1 / 46, loss: 26.52
val loss 33.024274701657504
current epoch:  56
validation batch 1 / 46, loss: 27.09
val loss 32.81458386130955
best epoch:  56
best val loss:  32.81458386130955
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_56.params
current epoch:  57
validation batch 1 / 46, loss: 27.36
val loss 33.88334506490956
current epoch:  58
validation batch 1 / 46, loss: 26.05
val loss 32.98776249263597
global step: 8000, training loss: 35.11, time: 4841.28s
current epoch:  59
validation batch 1 / 46, loss: 26.98
val loss 33.01424383080524
current epoch:  60
validation batch 1 / 46, loss: 26.38
val loss 33.170444571453594
current epoch:  61
validation batch 1 / 46, loss: 25.78
val loss 32.44919101051662
best epoch:  61
best val loss:  32.44919101051662
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_61.params
current epoch:  62
validation batch 1 / 46, loss: 26.73
val loss 32.80874824523926
current epoch:  63
validation batch 1 / 46, loss: 25.99
val loss 32.49702793618907
current epoch:  64
validation batch 1 / 46, loss: 26.84
val loss 32.80385079591171
current epoch:  65
validation batch 1 / 46, loss: 25.57
val loss 32.310823606408164
best epoch:  65
best val loss:  32.310823606408164
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_65.params
current epoch:  66
validation batch 1 / 46, loss: 26.00
val loss 32.60430895763895
global step: 9000, training loss: 32.04, time: 5424.68s
current epoch:  67
validation batch 1 / 46, loss: 26.47
val loss 32.78597616112751
current epoch:  68
validation batch 1 / 46, loss: 25.52
val loss 32.29597692904265
best epoch:  68
best val loss:  32.29597692904265
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_68.params
current epoch:  69
validation batch 1 / 46, loss: 25.48
val loss 32.40522509035857
current epoch:  70
validation batch 1 / 46, loss: 25.59
val loss 32.33253561932108
current epoch:  71
validation batch 1 / 46, loss: 25.04
val loss 31.71717009337052
best epoch:  71
best val loss:  31.71717009337052
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_71.params
current epoch:  72
validation batch 1 / 46, loss: 25.88
val loss 31.949052313099738
current epoch:  73
validation batch 1 / 46, loss: 25.39
val loss 31.6645160343336
best epoch:  73
best val loss:  31.6645160343336
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_73.params
global step: 10000, training loss: 28.39, time: 5998.38s
current epoch:  74
validation batch 1 / 46, loss: 25.96
val loss 32.226221996804945
current epoch:  75
validation batch 1 / 46, loss: 24.92
val loss 31.566832708275836
best epoch:  75
best val loss:  31.566832708275836
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_75.params
current epoch:  76
validation batch 1 / 46, loss: 25.37
val loss 31.985636089159094
current epoch:  77
validation batch 1 / 46, loss: 25.17
val loss 31.552907529084578
best epoch:  77
best val loss:  31.552907529084578
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_77.params
current epoch:  78
validation batch 1 / 46, loss: 25.76
val loss 31.80754441800325
current epoch:  79
validation batch 1 / 46, loss: 24.79
val loss 31.4355293356854
best epoch:  79
best val loss:  31.4355293356854
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_79.params
current epoch:  80
validation batch 1 / 46, loss: 24.91
val loss 31.425250633903172
best epoch:  80
best val loss:  31.425250633903172
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_80.params
global step: 11000, training loss: 29.85, time: 6573.08s
current epoch:  81
validation batch 1 / 46, loss: 24.77
val loss 31.38894155751104
best epoch:  81
best val loss:  31.38894155751104
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_81.params
current epoch:  82
validation batch 1 / 46, loss: 25.29
val loss 31.656030323194422
current epoch:  83
validation batch 1 / 46, loss: 25.18
val loss 31.34159623021665
best epoch:  83
best val loss:  31.34159623021665
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_83.params
current epoch:  84
validation batch 1 / 46, loss: 24.98
val loss 31.62788834779159
current epoch:  85
validation batch 1 / 46, loss: 24.32
val loss 31.241144926651664
best epoch:  85
best val loss:  31.241144926651664
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_85.params
current epoch:  86
validation batch 1 / 46, loss: 24.30
val loss 31.19263682158097
best epoch:  86
best val loss:  31.19263682158097
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_86.params
current epoch:  87
validation batch 1 / 46, loss: 24.65
val loss 31.473691484202508
current epoch:  88
validation batch 1 / 46, loss: 25.59
val loss 31.957335637963336
global step: 12000, training loss: 29.84, time: 7156.28s
current epoch:  89
validation batch 1 / 46, loss: 24.28
val loss 31.17719716611116
best epoch:  89
best val loss:  31.17719716611116
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_89.params
current epoch:  90
validation batch 1 / 46, loss: 24.37
val loss 30.974851525348164
best epoch:  90
best val loss:  30.974851525348164
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_90.params
current epoch:  91
validation batch 1 / 46, loss: 24.42
val loss 31.22767518914264
current epoch:  92
validation batch 1 / 46, loss: 26.06
val loss 31.866409467614215
current epoch:  93
validation batch 1 / 46, loss: 24.45
val loss 31.018240016439687
current epoch:  94
validation batch 1 / 46, loss: 24.20
val loss 31.31732409933339
current epoch:  95
validation batch 1 / 46, loss: 24.14
val loss 30.93105307869289
best epoch:  95
best val loss:  30.93105307869289
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_95.params
global step: 13000, training loss: 27.35, time: 7721.88s
current epoch:  96
validation batch 1 / 46, loss: 24.78
val loss 31.068011532659117
current epoch:  97
validation batch 1 / 46, loss: 24.39
val loss 31.013621703438137
current epoch:  98
validation batch 1 / 46, loss: 24.22
val loss 30.764773202979047
best epoch:  98
best val loss:  30.764773202979047
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_98.params
current epoch:  99
validation batch 1 / 46, loss: 24.40
val loss 30.967904588450555
best epoch: 98
Average Training Time: 71.0091 secs/epoch
Average Inference Time: 9.5648 secs
best epoch: 98
load weight from: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_98.params
predicting data set batch 1 / 46
input: (2898, 277, 1, 4)
prediction: (2898, 277, 4)
data_target_tensor: (2898, 277, 4)
current epoch: 98, predict 1-th point
MAE: 27.63
RMSE: 44.42
MAPE: 17.61
current epoch: 98, predict 2-th point
MAE: 30.44
RMSE: 49.37
MAPE: 19.83
current epoch: 98, predict 3-th point
MAE: 32.70
RMSE: 52.88
MAPE: 22.39
current epoch: 98, predict 4-th point
MAE: 36.45
RMSE: 58.33
MAPE: 25.52
all MAE: 31.81
all RMSE: 51.50
all MAPE: 21.34
[27.630524, 44.4154731251361, 17.608922719955444, 30.444328, 49.36830205756397, 19.832247495651245, 32.69867, 52.88304967907368, 22.385355830192566, 36.450764, 58.32795548126987, 25.52410662174225, 31.806051, 51.4993434234542, 21.337653696537018]

Process finished with exit code 0
