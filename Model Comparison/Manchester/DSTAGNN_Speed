ssh://root@edgegpu-proxy-002.gpumall.com:36513/usr/local/miniconda3/bin/python -u "/project/DSTAGNN-main -m/train_DSTAGNN_my.py"
Read configuration file: configurations/Manchester_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/Manchester/Manchester_r1_d0_w0_dstagnn
train: torch.Size([8693, 277, 1, 4]) torch.Size([8693, 277, 4])
val: torch.Size([2898, 277, 1, 4]) torch.Size([2898, 277, 4])
test: torch.Size([2898, 277, 1, 4]) torch.Size([2898, 277, 4])
delete the old one and create params directory myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 64
graph_signal_matrix_filename	 ./data/Manchester/Manchester.npz
start_epoch	 0
epochs	 100
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(4, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(4, 277)
        (norm): LayerNorm((277,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(277, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=277, out_features=96, bias=False)
        (W_K): Linear(in_features=277, out_features=96, bias=False)
        (W_V): Linear(in_features=277, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=277, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 277x277 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(16, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=4, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 4, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.0.EmbedT.norm.weight 	 torch.Size([277])
BlockList.0.EmbedT.norm.bias 	 torch.Size([277])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.0.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.0.fcmy.0.bias 	 torch.Size([4])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.1.EmbedT.norm.weight 	 torch.Size([277])
BlockList.1.EmbedT.norm.bias 	 torch.Size([277])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.1.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.1.fcmy.0.bias 	 torch.Size([4])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.2.EmbedT.norm.weight 	 torch.Size([277])
BlockList.2.EmbedT.norm.bias 	 torch.Size([277])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.2.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.2.fcmy.0.bias 	 torch.Size([4])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 4, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([4, 277])
BlockList.3.EmbedT.norm.weight 	 torch.Size([277])
BlockList.3.EmbedT.norm.bias 	 torch.Size([277])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([277, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 277])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 277])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 277])
BlockList.3.TAt.fc.weight 	 torch.Size([277, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([277, 277])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([277, 277])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([277, 277])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([4, 2])
BlockList.3.fcmy.0.bias 	 torch.Size([4])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 16, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([4, 128])
final_fc.bias 	 torch.Size([4])
Net's total params: 2622040
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]}]
current epoch:  0
validation batch 1 / 46, loss: 89.65
val loss 89.98236100570016
best epoch:  0
best val loss:  89.98236100570016
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 46, loss: 44.93
val loss 45.39900953873344
best epoch:  1
best val loss:  45.39900953873344
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 46, loss: 4.42
val loss 4.576228131418643
best epoch:  2
best val loss:  4.576228131418643
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 46, loss: 3.84
val loss 3.8768505091252536
best epoch:  3
best val loss:  3.8768505091252536
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 46, loss: 3.86
val loss 3.8425742983818054
best epoch:  4
best val loss:  3.8425742983818054
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 46, loss: 3.43
val loss 3.473440294680388
best epoch:  5
best val loss:  3.473440294680388
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 46, loss: 3.43
val loss 3.457411315130151
best epoch:  6
best val loss:  3.457411315130151
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
current epoch:  7
validation batch 1 / 46, loss: 3.32
val loss 3.358808061350947
best epoch:  7
best val loss:  3.358808061350947
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
global step: 1000, training loss: 2.95, time: 293.56s
current epoch:  8
validation batch 1 / 46, loss: 3.14
val loss 3.2071679830551147
best epoch:  8
best val loss:  3.2071679830551147
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 46, loss: 3.05
val loss 3.136431406373563
best epoch:  9
best val loss:  3.136431406373563
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 46, loss: 3.03
val loss 3.1066314578056335
best epoch:  10
best val loss:  3.1066314578056335
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 46, loss: 2.97
val loss 3.035266207612079
best epoch:  11
best val loss:  3.035266207612079
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 46, loss: 2.95
val loss 2.9997796390367593
best epoch:  12
best val loss:  2.9997796390367593
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
current epoch:  13
validation batch 1 / 46, loss: 2.91
val loss 2.9736503777296646
best epoch:  13
best val loss:  2.9736503777296646
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_13.params
current epoch:  14
validation batch 1 / 46, loss: 2.90
val loss 2.9511386441147844
best epoch:  14
best val loss:  2.9511386441147844
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
global step: 2000, training loss: 3.27, time: 565.55s
current epoch:  15
validation batch 1 / 46, loss: 2.89
val loss 2.9414022720378377
best epoch:  15
best val loss:  2.9414022720378377
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 46, loss: 2.85
val loss 2.8910238613253054
best epoch:  16
best val loss:  2.8910238613253054
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_16.params
current epoch:  17
validation batch 1 / 46, loss: 2.86
val loss 2.9014108880706457
current epoch:  18
validation batch 1 / 46, loss: 2.80
val loss 2.8519738990327586
best epoch:  18
best val loss:  2.8519738990327586
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_18.params
current epoch:  19
validation batch 1 / 46, loss: 2.82
val loss 2.840761477532594
best epoch:  19
best val loss:  2.840761477532594
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 46, loss: 2.80
val loss 2.8566696436508843
current epoch:  21
validation batch 1 / 46, loss: 2.81
val loss 2.850511854109557
current epoch:  22
validation batch 1 / 46, loss: 2.78
val loss 2.8182679129683454
best epoch:  22
best val loss:  2.8182679129683454
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_22.params
global step: 3000, training loss: 2.91, time: 838.49s
current epoch:  23
validation batch 1 / 46, loss: 2.78
val loss 2.8299785463706306
current epoch:  24
validation batch 1 / 46, loss: 2.76
val loss 2.805128320403721
best epoch:  24
best val loss:  2.805128320403721
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
current epoch:  25
validation batch 1 / 46, loss: 2.73
val loss 2.777993292912193
best epoch:  25
best val loss:  2.777993292912193
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_25.params
current epoch:  26
validation batch 1 / 46, loss: 2.79
val loss 2.835253049498019
current epoch:  27
validation batch 1 / 46, loss: 2.75
val loss 2.7931903704353003
current epoch:  28
validation batch 1 / 46, loss: 2.73
val loss 2.8006718080976736
current epoch:  29
validation batch 1 / 46, loss: 2.75
val loss 2.808945650639741
global step: 4000, training loss: 2.81, time: 1109.18s
current epoch:  30
validation batch 1 / 46, loss: 2.69
val loss 2.751132389773493
best epoch:  30
best val loss:  2.751132389773493
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_30.params
current epoch:  31
validation batch 1 / 46, loss: 2.71
val loss 2.7541747870652573
current epoch:  32
validation batch 1 / 46, loss: 2.75
val loss 2.803989612537882
current epoch:  33
validation batch 1 / 46, loss: 2.76
val loss 2.811207307421643
current epoch:  34
validation batch 1 / 46, loss: 2.74
val loss 2.76976738287055
current epoch:  35
validation batch 1 / 46, loss: 2.67
val loss 2.7182038322738977
best epoch:  35
best val loss:  2.7182038322738977
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_35.params
current epoch:  36
validation batch 1 / 46, loss: 2.71
val loss 2.766002893447876
global step: 5000, training loss: 2.53, time: 1381.48s
current epoch:  37
validation batch 1 / 46, loss: 2.67
val loss 2.738238241361535
current epoch:  38
validation batch 1 / 46, loss: 2.66
val loss 2.7200976402863213
current epoch:  39
validation batch 1 / 46, loss: 2.66
val loss 2.7383989432583684
current epoch:  40
validation batch 1 / 46, loss: 2.66
val loss 2.7471120072447737
current epoch:  41
validation batch 1 / 46, loss: 2.59
val loss 2.673792802769205
best epoch:  41
best val loss:  2.673792802769205
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_41.params
current epoch:  42
validation batch 1 / 46, loss: 2.62
val loss 2.6828537946162014
current epoch:  43
validation batch 1 / 46, loss: 2.61
val loss 2.6886777955552805
current epoch:  44
validation batch 1 / 46, loss: 2.59
val loss 2.6971298197041387
global step: 6000, training loss: 2.33, time: 1657.02s
current epoch:  45
validation batch 1 / 46, loss: 2.59
val loss 2.6773484375165855
current epoch:  46
validation batch 1 / 46, loss: 2.61
val loss 2.70144536184228
current epoch:  47
validation batch 1 / 46, loss: 2.60
val loss 2.711855471134186
current epoch:  48
validation batch 1 / 46, loss: 2.67
val loss 2.7456698702729265
current epoch:  49
validation batch 1 / 46, loss: 2.63
val loss 2.710040649642115
current epoch:  50
validation batch 1 / 46, loss: 2.58
val loss 2.674156930135644
current epoch:  51
validation batch 1 / 46, loss: 2.65
val loss 2.7402816492578257
global step: 7000, training loss: 2.52, time: 1953.40s
current epoch:  52
validation batch 1 / 46, loss: 2.62
val loss 2.719403191753056
current epoch:  53
validation batch 1 / 46, loss: 2.60
val loss 2.7148128722025002
current epoch:  54
validation batch 1 / 46, loss: 2.57
val loss 2.6588395730308862
best epoch:  54
best val loss:  2.6588395730308862
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_54.params
current epoch:  55
validation batch 1 / 46, loss: 2.56
val loss 2.672988930474157
current epoch:  56
validation batch 1 / 46, loss: 2.60
val loss 2.7204257431237595
current epoch:  57
validation batch 1 / 46, loss: 2.60
val loss 2.7148486634959346
current epoch:  58
validation batch 1 / 46, loss: 2.54
val loss 2.6531261179758157
best epoch:  58
best val loss:  2.6531261179758157
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_58.params
global step: 8000, training loss: 2.64, time: 2250.32s
current epoch:  59
validation batch 1 / 46, loss: 2.53
val loss 2.6607282420863276
current epoch:  60
validation batch 1 / 46, loss: 2.53
val loss 2.6545059836429097
current epoch:  61
validation batch 1 / 46, loss: 2.53
val loss 2.6507286351660024
best epoch:  61
best val loss:  2.6507286351660024
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_61.params
current epoch:  62
validation batch 1 / 46, loss: 2.60
val loss 2.728939414024353
current epoch:  63
validation batch 1 / 46, loss: 2.53
val loss 2.6583043077717656
current epoch:  64
validation batch 1 / 46, loss: 2.57
val loss 2.6907030732735344
current epoch:  65
validation batch 1 / 46, loss: 2.58
val loss 2.730103497919829
current epoch:  66
validation batch 1 / 46, loss: 2.56
val loss 2.6834806488907854
global step: 9000, training loss: 2.60, time: 2550.96s
current epoch:  67
validation batch 1 / 46, loss: 2.63
val loss 2.7522345962731736
current epoch:  68
validation batch 1 / 46, loss: 2.56
val loss 2.691412752089293
current epoch:  69
validation batch 1 / 46, loss: 2.51
val loss 2.6663467780403467
current epoch:  70
validation batch 1 / 46, loss: 2.51
val loss 2.644305742305258
best epoch:  70
best val loss:  2.644305742305258
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_70.params
current epoch:  71
validation batch 1 / 46, loss: 2.52
val loss 2.6638103049734365
current epoch:  72
validation batch 1 / 46, loss: 2.48
val loss 2.6314104380814927
best epoch:  72
best val loss:  2.6314104380814927
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_72.params
current epoch:  73
validation batch 1 / 46, loss: 2.51
val loss 2.6384702428527502
global step: 10000, training loss: 2.60, time: 2847.88s
current epoch:  74
validation batch 1 / 46, loss: 2.54
val loss 2.6603922403377034
current epoch:  75
validation batch 1 / 46, loss: 2.48
val loss 2.637271365393763
current epoch:  76
validation batch 1 / 46, loss: 2.51
val loss 2.6577817927236143
current epoch:  77
validation batch 1 / 46, loss: 2.53
val loss 2.662942049296006
current epoch:  78
validation batch 1 / 46, loss: 2.50
val loss 2.6474603803261467
current epoch:  79
validation batch 1 / 46, loss: 2.52
val loss 2.6358380939649497
current epoch:  80
validation batch 1 / 46, loss: 2.51
val loss 2.648751805657926
global step: 11000, training loss: 2.48, time: 3146.27s
current epoch:  81
validation batch 1 / 46, loss: 2.51
val loss 2.645715003428252
current epoch:  82
validation batch 1 / 46, loss: 2.49
val loss 2.6291493643885073
best epoch:  82
best val loss:  2.6291493643885073
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_82.params
current epoch:  83
validation batch 1 / 46, loss: 2.48
val loss 2.635755761809971
current epoch:  84
validation batch 1 / 46, loss: 2.47
val loss 2.635206559429998
current epoch:  85
validation batch 1 / 46, loss: 2.48
val loss 2.629117294498112
best epoch:  85
best val loss:  2.629117294498112
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_85.params
current epoch:  86
validation batch 1 / 46, loss: 2.48
val loss 2.643216804317806
current epoch:  87
validation batch 1 / 46, loss: 2.49
val loss 2.6310837864875793
current epoch:  88
validation batch 1 / 46, loss: 2.49
val loss 2.6397764242213704
global step: 12000, training loss: 2.24, time: 3450.37s
current epoch:  89
validation batch 1 / 46, loss: 2.50
val loss 2.656162624773772
current epoch:  90
validation batch 1 / 46, loss: 2.49
val loss 2.6273755923561426
best epoch:  90
best val loss:  2.6273755923561426
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_90.params
current epoch:  91
validation batch 1 / 46, loss: 2.48
val loss 2.6313738175060437
current epoch:  92
validation batch 1 / 46, loss: 2.49
val loss 2.6355692822000254
current epoch:  93
validation batch 1 / 46, loss: 2.50
val loss 2.6256118935087454
best epoch:  93
best val loss:  2.6256118935087454
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_93.params
current epoch:  94
validation batch 1 / 46, loss: 2.49
val loss 2.6311166597449263
current epoch:  95
validation batch 1 / 46, loss: 2.49
val loss 2.6201557568881824
best epoch:  95
best val loss:  2.6201557568881824
save parameters to file: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_95.params
global step: 13000, training loss: 2.70, time: 3723.27s
current epoch:  96
validation batch 1 / 46, loss: 2.50
val loss 2.6376162015873454
current epoch:  97
validation batch 1 / 46, loss: 2.51
val loss 2.6426252359929294
current epoch:  98
validation batch 1 / 46, loss: 2.51
val loss 2.6276612800100576
current epoch:  99
validation batch 1 / 46, loss: 2.48
val loss 2.6305033331331997
best epoch: 95
Average Training Time: 34.2268 secs/epoch
Average Inference Time: 4.6070 secs
best epoch: 95
load weight from: myexperiments/Manchester/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_95.params
predicting data set batch 1 / 46
input: (2898, 277, 1, 4)
prediction: (2898, 277, 4)
data_target_tensor: (2898, 277, 4)
current epoch: 95, predict 1-th point
MAE: 2.73
RMSE: 6.75
MAPE: 4.19
current epoch: 95, predict 2-th point
MAE: 3.28
RMSE: 8.33
MAPE: 5.20
current epoch: 95, predict 3-th point
MAE: 3.64
RMSE: 9.28
MAPE: 5.83
current epoch: 95, predict 4-th point
MAE: 3.97
RMSE: 10.04
MAPE: 6.42
all MAE: 3.40
all RMSE: 8.69
all MAPE: 5.41
[2.7275677, 6.750287650301757, 4.194063693284988, 3.2755396, 8.327224425099672, 5.199402570724487, 3.6378515, 9.282227333064803, 5.832448974251747, 3.9746697, 10.037153651816624, 6.424584984779358, 3.403907, 8.686387244663791, 5.412618070840836]

Process finished with exit code 0
