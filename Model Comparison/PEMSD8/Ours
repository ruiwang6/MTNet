ssh://root@region-42.seetacloud.com:33037/root/miniconda3/bin/python -u "/project/MTL-main - /model/train.py"
PEMS08
Train:  (10691, 12, 170, 4) (10691, 12, 170, 2)
Val:  (3548, 12, 170, 4) (3548, 12, 170, 2)
Test:  (3548, 12, 170, 4) (3548, 12, 170, 2)

--------- MTLSTformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 100,
    "early_stop": 15,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_emb_dim": 24,
        "tod_emb_dim": 24,
        "dow_emb_dim": 24,
        "adaptive_emb_dim": 80,
        "feed_forward_dim": 256,
        "cross_feed_forward_dim": 64,
        "cross_heads": 8,
        "num_heads": 4,
        "num_layers": 2,
        "dropout": 0.1
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MTLSTformer                                             [16, 12, 170, 2]          163,200
├─Linear: 1-1                                           [16, 12, 170, 24]         96
├─Linear: 1-2                                           [16, 12, 170, 24]         96
├─Embedding: 1-3                                        [16, 12, 170, 24]         6,912
├─Embedding: 1-4                                        [16, 12, 170, 24]         168
├─CrossTaskAttentionLayer: 1-5                          [16, 12, 170, 152]        --
│    └─AttentionLayer: 2-1                              [16, 170, 12, 152]        --
│    │    └─Linear: 3-1                                 [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-2                                 [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-3                                 [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-4                                 [16, 170, 12, 152]        23,256
│    └─Dropout: 2-2                                     [16, 170, 12, 152]        --
│    └─LayerNorm: 2-3                                   [16, 170, 12, 152]        304
│    └─Sequential: 2-4                                  [16, 170, 12, 152]        --
│    │    └─Linear: 3-5                                 [16, 170, 12, 64]         9,792
│    │    └─ReLU: 3-6                                   [16, 170, 12, 64]         --
│    │    └─Linear: 3-7                                 [16, 170, 12, 152]        9,880
│    └─Dropout: 2-5                                     [16, 170, 12, 152]        --
│    └─LayerNorm: 2-6                                   [16, 170, 12, 152]        304
├─CrossTaskAttentionLayer: 1-6                          [16, 12, 170, 152]        --
│    └─AttentionLayer: 2-7                              [16, 170, 12, 152]        --
│    │    └─Linear: 3-8                                 [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-9                                 [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-10                                [16, 170, 12, 152]        23,256
│    │    └─Linear: 3-11                                [16, 170, 12, 152]        23,256
│    └─Dropout: 2-8                                     [16, 170, 12, 152]        --
│    └─LayerNorm: 2-9                                   [16, 170, 12, 152]        304
│    └─Sequential: 2-10                                 [16, 170, 12, 152]        --
│    │    └─Linear: 3-12                                [16, 170, 12, 64]         9,792
│    │    └─ReLU: 3-13                                  [16, 170, 12, 64]         --
│    │    └─Linear: 3-14                                [16, 170, 12, 152]        9,880
│    └─Dropout: 2-11                                    [16, 170, 12, 152]        --
│    └─LayerNorm: 2-12                                  [16, 170, 12, 152]        304
├─ModuleList: 1-7                                       --                        --
│    └─SelfAttentionLayer: 2-13                         [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-15                        [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-16                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-17                             [16, 170, 12, 152]        304
│    │    └─Sequential: 3-18                            [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-19                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-20                             [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-14                         [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-21                        [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-22                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-23                             [16, 170, 12, 152]        304
│    │    └─Sequential: 3-24                            [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-25                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-26                             [16, 170, 12, 152]        304
├─ModuleList: 1-8                                       --                        --
│    └─SelfAttentionLayer: 2-15                         [16, 12, 170, 152]        --
│    │    └─SpatialAttentionConvlutionLayer: 3-27       [16, 12, 170, 152]        92,872
│    │    └─Dropout: 3-28                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-29                             [16, 12, 170, 152]        304
│    │    └─Sequential: 3-30                            [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-31                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-32                             [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-16                         [16, 12, 170, 152]        --
│    │    └─SpatialAttentionConvlutionLayer: 3-33       [16, 12, 170, 152]        92,872
│    │    └─Dropout: 3-34                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-35                             [16, 12, 170, 152]        304
│    │    └─Sequential: 3-36                            [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-37                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-38                             [16, 12, 170, 152]        304
├─ModuleList: 1-9                                       --                        --
│    └─SelfAttentionLayer: 2-17                         [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-39                        [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-40                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-41                             [16, 170, 12, 152]        304
│    │    └─Sequential: 3-42                            [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-43                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-44                             [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-18                         [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-45                        [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-46                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-47                             [16, 170, 12, 152]        304
│    │    └─Sequential: 3-48                            [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-49                               [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-50                             [16, 170, 12, 152]        304
├─ModuleList: 1-10                                      --                        --
│    └─SelfAttentionLayer: 2-19                         [16, 12, 170, 152]        --
│    │    └─SpatialAttentionConvlutionLayer: 3-51       [16, 12, 170, 152]        92,872
│    │    └─Dropout: 3-52                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-53                             [16, 12, 170, 152]        304
│    │    └─Sequential: 3-54                            [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-55                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-56                             [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-20                         [16, 12, 170, 152]        --
│    │    └─SpatialAttentionConvlutionLayer: 3-57       [16, 12, 170, 152]        92,872
│    │    └─Dropout: 3-58                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-59                             [16, 12, 170, 152]        304
│    │    └─Sequential: 3-60                            [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-61                               [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-62                             [16, 12, 170, 152]        304
├─Linear: 1-11                                          [16, 170, 12]             21,900
├─Linear: 1-12                                          [16, 170, 12]             21,900
=========================================================================================================
Total params: 1,815,184
Trainable params: 1,815,184
Non-trainable params: 0
Total mult-adds (M): 26.43
=========================================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 3372.10
Params size (MB): 6.61
Estimated Total Size (MB): 3379.23
=========================================================================================================

Loss: HuberLoss

2024-08-23 10:06:31.110320 Epoch 1  	Train Loss = 31.53983 Val Loss = 24.71673
2024-08-23 10:08:08.453209 Epoch 2  	Train Loss = 21.44990 Val Loss = 20.87905
2024-08-23 10:09:46.183691 Epoch 3  	Train Loss = 19.90406 Val Loss = 21.59977
2024-08-23 10:11:23.820176 Epoch 4  	Train Loss = 19.12720 Val Loss = 18.16934
2024-08-23 10:13:01.356444 Epoch 5  	Train Loss = 18.34790 Val Loss = 19.34005
2024-08-23 10:14:39.489238 Epoch 6  	Train Loss = 18.23135 Val Loss = 18.90343
2024-08-23 10:16:18.514826 Epoch 7  	Train Loss = 17.86042 Val Loss = 19.44279
2024-08-23 10:17:56.425522 Epoch 8  	Train Loss = 17.41302 Val Loss = 18.61543
2024-08-23 10:19:34.106823 Epoch 9  	Train Loss = 17.18239 Val Loss = 17.20685
2024-08-23 10:21:12.351844 Epoch 10  	Train Loss = 17.08164 Val Loss = 17.54503
2024-08-23 10:22:49.939063 Epoch 11  	Train Loss = 16.68849 Val Loss = 17.15482
2024-08-23 10:24:28.038382 Epoch 12  	Train Loss = 16.66784 Val Loss = 17.51327
2024-08-23 10:26:06.389670 Epoch 13  	Train Loss = 16.38028 Val Loss = 16.38831
2024-08-23 10:27:44.296494 Epoch 14  	Train Loss = 16.30800 Val Loss = 16.92070
2024-08-23 10:29:21.900914 Epoch 15  	Train Loss = 16.10178 Val Loss = 16.68845
2024-08-23 10:30:59.852636 Epoch 16  	Train Loss = 15.92041 Val Loss = 16.53593
2024-08-23 10:32:38.412476 Epoch 17  	Train Loss = 15.74979 Val Loss = 16.31343
2024-08-23 10:34:17.577327 Epoch 18  	Train Loss = 15.68861 Val Loss = 15.87686
2024-08-23 10:35:59.325517 Epoch 19  	Train Loss = 15.49197 Val Loss = 15.94091
2024-08-23 10:37:37.020523 Epoch 20  	Train Loss = 15.53212 Val Loss = 16.20511
2024-08-23 10:39:14.867332 Epoch 21  	Train Loss = 15.42103 Val Loss = 15.97444
2024-08-23 10:40:52.511848 Epoch 22  	Train Loss = 15.32323 Val Loss = 15.68495
2024-08-23 10:42:30.178108 Epoch 23  	Train Loss = 15.17670 Val Loss = 15.71869
2024-08-23 10:44:08.339627 Epoch 24  	Train Loss = 15.19925 Val Loss = 15.96563
2024-08-23 10:45:46.376647 Epoch 25  	Train Loss = 15.04551 Val Loss = 15.63519
2024-08-23 10:47:24.445470 Epoch 26  	Train Loss = 14.40765 Val Loss = 15.05258
2024-08-23 10:49:02.030259 Epoch 27  	Train Loss = 14.32714 Val Loss = 15.05150
2024-08-23 10:50:39.572146 Epoch 28  	Train Loss = 14.29467 Val Loss = 15.07370
2024-08-23 10:52:16.967173 Epoch 29  	Train Loss = 14.27261 Val Loss = 15.10177
2024-08-23 10:53:54.419562 Epoch 30  	Train Loss = 14.24966 Val Loss = 15.04596
2024-08-23 10:55:31.918317 Epoch 31  	Train Loss = 14.22724 Val Loss = 15.04082
2024-08-23 10:57:09.281612 Epoch 32  	Train Loss = 14.20492 Val Loss = 15.07537
2024-08-23 10:58:46.741692 Epoch 33  	Train Loss = 14.18696 Val Loss = 15.02758
2024-08-23 11:00:24.587660 Epoch 34  	Train Loss = 14.17191 Val Loss = 15.07410
2024-08-23 11:02:02.046658 Epoch 35  	Train Loss = 14.15018 Val Loss = 15.09094
2024-08-23 11:03:39.707305 Epoch 36  	Train Loss = 14.13697 Val Loss = 15.04095
2024-08-23 11:05:18.247712 Epoch 37  	Train Loss = 14.12355 Val Loss = 15.00437
2024-08-23 11:06:55.692409 Epoch 38  	Train Loss = 14.10888 Val Loss = 15.10017
2024-08-23 11:08:33.127024 Epoch 39  	Train Loss = 14.09213 Val Loss = 14.94809
2024-08-23 11:10:10.629783 Epoch 40  	Train Loss = 14.08249 Val Loss = 15.02682
2024-08-23 11:11:48.048913 Epoch 41  	Train Loss = 14.07111 Val Loss = 15.04475
2024-08-23 11:13:25.487585 Epoch 42  	Train Loss = 14.05993 Val Loss = 15.01252
2024-08-23 11:15:03.069171 Epoch 43  	Train Loss = 14.04302 Val Loss = 15.01985
2024-08-23 11:16:40.590486 Epoch 44  	Train Loss = 14.03232 Val Loss = 15.04234
2024-08-23 11:18:18.756820 Epoch 45  	Train Loss = 14.02337 Val Loss = 15.00092
2024-08-23 11:19:56.136387 Epoch 46  	Train Loss = 13.94886 Val Loss = 14.96984
2024-08-23 11:21:33.575124 Epoch 47  	Train Loss = 13.94028 Val Loss = 14.97239
2024-08-23 11:23:11.173040 Epoch 48  	Train Loss = 13.93652 Val Loss = 14.95904
2024-08-23 11:24:48.617548 Epoch 49  	Train Loss = 13.93504 Val Loss = 14.97388
2024-08-23 11:26:26.052562 Epoch 50  	Train Loss = 13.93473 Val Loss = 14.97839
2024-08-23 11:28:03.349846 Epoch 51  	Train Loss = 13.93119 Val Loss = 14.96985
2024-08-23 11:29:40.836147 Epoch 52  	Train Loss = 13.92894 Val Loss = 14.96255
2024-08-23 11:31:18.228590 Epoch 53  	Train Loss = 13.92902 Val Loss = 14.96713
2024-08-23 11:32:55.630031 Epoch 54  	Train Loss = 13.92512 Val Loss = 14.96135
Early stopping at epoch: 54
Best at epoch 39:
Train Loss = 14.09213
Train RMSE = 15.97063, MAE = 6.90745, MAPE = 5.28753
Val Loss = 14.94809
Val RMSE = 16.98317, MAE = 7.36557, MAPE = 6.26653
Saved Model: ../saved_models/MTLSTformer-PEMS08-2024-08-23-10-04-42.pt
--------- Test ---------
Flow All Steps RMSE = 22.88067, MAE = 13.31114, MAPE = 8.75096
Flow Step 1 RMSE = 19.53469, MAE = 11.73786, MAPE = 7.79378
Flow Step 2 RMSE = 20.50874, MAE = 12.13984, MAPE = 8.02450
Flow Step 3 RMSE = 21.29434, MAE = 12.49144, MAPE = 8.21913
Flow Step 4 RMSE = 21.93538, MAE = 12.78389, MAPE = 8.40725
Flow Step 5 RMSE = 22.45578, MAE = 13.04735, MAPE = 8.56440
Flow Step 6 RMSE = 22.93228, MAE = 13.29546, MAPE = 8.71886
Flow Step 7 RMSE = 23.36382, MAE = 13.52438, MAPE = 8.86369
Flow Step 8 RMSE = 23.73472, MAE = 13.73755, MAPE = 9.00501
Flow Step 9 RMSE = 24.04920, MAE = 13.93244, MAPE = 9.14817
Flow Step 10 RMSE = 24.35668, MAE = 14.12157, MAPE = 9.25650
Flow Step 11 RMSE = 24.66140, MAE = 14.33341, MAPE = 9.41802
Flow Step 12 RMSE = 25.02649, MAE = 14.58855, MAPE = 9.59229
Speed All Steps RMSE = 3.29415, MAE = 1.31011, MAPE = 2.84084
Speed Step 1 RMSE = 1.51681, MAE = 0.76619, MAPE = 1.46300
Speed Step 2 RMSE = 2.10145, MAE = 0.98052, MAPE = 1.92536
Speed Step 3 RMSE = 2.53160, MAE = 1.11825, MAPE = 2.25735
Speed Step 4 RMSE = 2.86832, MAE = 1.21861, MAPE = 2.53476
Speed Step 5 RMSE = 3.13555, MAE = 1.29459, MAPE = 2.75493
Speed Step 6 RMSE = 3.34823, MAE = 1.35501, MAPE = 2.93410
Speed Step 7 RMSE = 3.52537, MAE = 1.40295, MAPE = 3.08637
Speed Step 8 RMSE = 3.66840, MAE = 1.44625, MAPE = 3.21223
Speed Step 9 RMSE = 3.78697, MAE = 1.48264, MAPE = 3.32410
Speed Step 10 RMSE = 3.89100, MAE = 1.51660, MAPE = 3.42781
Speed Step 11 RMSE = 3.98400, MAE = 1.55018, MAPE = 3.53156
Speed Step 12 RMSE = 4.07228, MAE = 1.58950, MAPE = 3.63847
Inference time: 9.39 s

Process finished with exit code -1
